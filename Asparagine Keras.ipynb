{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4948006c",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Regression Example With Boston Dataset: Standardized and Wider\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import keras\n",
    "import keras.utils\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "from cgi import test\n",
    "\n",
    "dataset = pd.read_csv('aggregated_data.csv')\n",
    "dataset = shuffle(dataset)\n",
    "\n",
    "std_scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a260138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importData(data, scaler):\n",
    "\n",
    "    train_dataset = data.sample(frac=0.8, random_state=9578)\n",
    "    test_dataset = data.drop(train_dataset.index)\n",
    "\n",
    "\n",
    "    train_features = train_dataset.copy()\n",
    "    test_features = test_dataset.copy()\n",
    "\n",
    "\n",
    "    train_labels = train_features.pop('Concentration')\n",
    "    test_labels = test_features.pop('Concentration')\n",
    "\n",
    "    train_features = scaler.fit_transform(train_features.to_numpy())\n",
    "    dict = {'Time':train_features[:, 0], 'Current':train_features[:, 1], 'Spin Coating':train_features[:, 2] ,'Increaing PPM':train_features[:, 3], 'Temperature':train_features[:, 4], 'Repeat Sensor Use':train_features[:, 5], 'Days Elapsed':train_features[:, 6]}\n",
    "    train_features = pd.DataFrame(dict)\n",
    "\n",
    "    test_features = scaler.fit_transform(test_features.to_numpy())\n",
    "    dict = {'Time':test_features[:, 0], 'Current':test_features[:, 1], 'Spin Coating':test_features[:, 2] ,'Increaing PPM':test_features[:, 3], 'Temperature':test_features[:, 4], 'Repeat Sensor Use':test_features[:, 5], 'Days Elapsed':test_features[:, 6]}\n",
    "    test_features = pd.DataFrame(dict)\n",
    "\n",
    "    #For later use\n",
    "    data_labels = data.pop('Concentration')\n",
    "\n",
    "    return data, train_features, test_features, train_labels, test_labels, data_labels\n",
    "#sns.pairplot(train_dataset[['Time','Current', 'Spin Coating', 'Increasing PPM', 'Temperature', 'Repeat Sensor Use', 'Days Elapsed', 'Concentration']], diag_kind='kde')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c253350f",
   "metadata": {},
   "source": [
    "#### Plotting Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97503096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "288cdd69",
   "metadata": {},
   "source": [
    "# Neural Network Creation and Selection Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db1ec8d",
   "metadata": {},
   "source": [
    "### Functions: Build NN Model, Fit Model, K Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a67997",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate through this a few dozen times\n",
    "\n",
    "def build_model(n1, n2, train_feats):\n",
    "  #Experiment with different models, thicknesses, layers, activation functions; Don't limit to only 10 nodes; Measure up to 64 nodes in 2 layers\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(n1, activation=tf.nn.relu, input_shape=[len(train_feats.keys())]),\n",
    "    layers.Dense(n2, activation=tf.nn.relu),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "  model.compile(loss='mse', optimizer=optimizer, metrics=['mae','mse'])\n",
    "  early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',patience=5)\n",
    "\n",
    "  return model\n",
    "\n",
    "def model_history(features, labels, model, epo, batch, vbs):\n",
    "  \n",
    "    history = model.fit(\n",
    "        features, labels,\n",
    "        epochs=epo, batch_size=batch, validation_split=0.2, verbose=vbs #, callbacks=early_stop\n",
    "    )\n",
    "\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    \n",
    "    return hist\n",
    "\n",
    "def KCrossValidation(i, features, labels, num_val_samples, epochs, batch, verbose, n1, n2):\n",
    "\n",
    "    print('processing fold #', i)\n",
    "    val_data = features[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = labels[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    partial_train_data = np.concatenate([features[:i * num_val_samples], features[(i + 1) * num_val_samples:]], axis=0)\n",
    "    partial_train_targets = np.concatenate([labels[:i * num_val_samples], labels[(i + 1) * num_val_samples:]],     axis=0)\n",
    "\n",
    "    model = build_model(n1, n2, features)\n",
    "\n",
    "    history = model_history(partial_train_data, partial_train_targets, model, epochs, batch, verbose)\n",
    "\n",
    "    test_loss, test_mae, test_mse = model.evaluate(val_data, val_targets, verbose=1)\n",
    "\n",
    "    return model, history, test_loss, test_mae, test_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80992c7e",
   "metadata": {},
   "source": [
    "## NEURAL NETWORK PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88108858",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features, train_features, test_features, train_labels, test_labels, data_labels = importData(dataset.copy(), std_scaler)\n",
    "\n",
    "k_folds = 3\n",
    "num_val_samples = len(train_labels) // k_folds\n",
    "\n",
    "n1_start = 32\n",
    "n2_start = 31\n",
    "sum_nodes = 64\n",
    "\n",
    "num_epochs = 300\n",
    "batch_size = 100\n",
    "verbose = 0\n",
    "avg_val_scores = []\n",
    "order_of_architecture = []\n",
    "\n",
    "all_networks  = []\n",
    "all_history  = []\n",
    "mae_history = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02d5d52",
   "metadata": {},
   "source": [
    "##### Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a207366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "\n",
    "  plt.plot(history['loss'], label='loss')\n",
    "  plt.plot(history['val_loss'], label='val_loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "def correlation_plots(model, label, input_data, title, xlabel, ylabel):\n",
    "#test_loss, test_acc = model.evaluate(test_features, test_labels, verbose = 1)\n",
    "\n",
    "  test_predictions = model.predict(input_data).flatten()\n",
    "  plt.scatter(label,test_predictions)\n",
    "  plt.xlabel(xlabel)\n",
    "  plt.ylabel(ylabel)\n",
    "  plt.title(title)\n",
    "  plt.axis('equal')\n",
    "  plt.axis('square')\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "  return test_predictions\n",
    "\n",
    "\n",
    "def plotGraph(y_test, y_pred,regressorName):\n",
    "    plt.scatter(range(len(y_pred)), y_test, color='blue')\n",
    "    plt.scatter(range(len(y_pred)), y_pred, color='red')\n",
    "    plt.title(regressorName)\n",
    "    plt.show()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5bcb0c",
   "metadata": {},
   "source": [
    "#### Where the Magic Happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83bf62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(TAKEN FROM DEEP LEARNING WITH PYTHON BY MANNING)\n",
    "for i in range(n1_start, sum_nodes):\n",
    "\n",
    "    for j in range(n2_start, sum_nodes):\n",
    "        if (i+j > sum_nodes):\n",
    "            continue\n",
    "        \n",
    "        print(\"first hidden layer\", j)\n",
    "        print(\"second hidden layer\", i)\n",
    "        k_fold_test_scores = []\n",
    "        k_models = []\n",
    "        k_history = []\n",
    "\n",
    "        k_mae_history = []\n",
    "\n",
    "        for fold in range(k_folds):\n",
    "            model, history, test_loss, test_mae, test_mse = KCrossValidation(\n",
    "                fold, \n",
    "                train_features, \n",
    "                train_labels, \n",
    "                num_val_samples, \n",
    "                num_epochs, \n",
    "                batch_size, \n",
    "                verbose, \n",
    "                j, \n",
    "                i)\n",
    "\n",
    "            #plot_loss(history)\n",
    "            k_fold_test_scores.append(test_mae)\n",
    "            k_history.append(history)\n",
    "            k_models.append(model)\n",
    "            k_mae_history.append(history['val_mae'])\n",
    "\n",
    "\n",
    "        avg_val_scores.append(sum(k_fold_test_scores)/len(k_fold_test_scores))\n",
    "        all_history.append(k_history)\n",
    "        all_networks.append(k_models)\n",
    "\n",
    "        \n",
    "        mae_history.append([ np.mean([x[i] for x in k_mae_history]) for i in range(num_epochs)])\n",
    "\n",
    "\n",
    "        order_of_architecture.append([i, j])\n",
    "\n",
    "\n",
    "       #test_predictions = correlation_plots(model, test_labels, test_features, \"Testing Correlation Plot\", \"Actual\", \"Predicted\")\n",
    "        #plotGraph(test_labels, test_predictions, \"Testing Plot\")\n",
    "\n",
    "\n",
    "        #training_predictions = correlation_plots(model, train_labels, train_features, \"Training Correlation Plot\", \"Actual\", \"Predicted\")\n",
    "        #plotGraph(train_labels, training_predictions, \"Training Plot\")\n",
    "\n",
    "# Find the model with the lowest error\n",
    "lowest_index = avg_val_scores.index(min(avg_val_scores))\n",
    "optimal_NNs = all_networks[lowest_index]\n",
    "\n",
    "print(avg_val_scores)\n",
    "\n",
    "#print(mae_history)\n",
    "# Find the history of that model, and display it\n",
    "for i in range(k_folds):\n",
    "    x = all_history[lowest_index][i]['val_mae']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fb47e2",
   "metadata": {},
   "source": [
    "Plotting Loss Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a1780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor=0.8):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points\n",
    "\n",
    "plt.plot(range(1, len(mae_history[lowest_index][int(num_epochs/10):]) + 1), mae_history[lowest_index][int(num_epochs/10):])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()\n",
    "\n",
    "smooth_mae_history = smooth_curve(mae_history[lowest_index][int(num_epochs/10):])\n",
    "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d8e06f",
   "metadata": {},
   "source": [
    "# Isolating Parameters and Printing them Out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4877308",
   "metadata": {},
   "source": [
    "Scaling Data Set Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e1152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleDataset(data):\n",
    "    data = std_scaler.fit_transform(data.to_numpy())\n",
    "    dict = {'Time':data[:, 0], 'Current':data[:, 1], 'Spin Coating':data[:, 2] ,'Increaing PPM':data[:, 3], 'Temperature':data[:, 4], 'Repeat Sensor Use':data[:, 5], 'Days Elapsed':data[:, 6]}\n",
    "    return pd.DataFrame(dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d19d04",
   "metadata": {},
   "source": [
    "### Isolating Spin Coating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea7f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data labels with spin coating 0 and 1\n",
    "\n",
    "sc_index = [np.where(dataset['Spin Coating'].to_numpy()  == 0)[0], np.where(dataset['Spin Coating'].to_numpy()  == 1)[0]]\n",
    "\n",
    "scaled_features = scaleDataset(all_features.copy())\n",
    "\n",
    "#The full features of the data points that use Spin Coating\n",
    "sc_features = [scaled_features.iloc[sc_index[0]], scaled_features.iloc[sc_index[1]]]\n",
    "\n",
    "#The stupid labels for Spin coating vs. not Spin coating\n",
    "sc_label = [data_labels.to_numpy()[sc_index[0]], data_labels.to_numpy()[sc_index[1]]]\n",
    "\n",
    "sc_mae = []\n",
    "for i in range(0, 2):\n",
    "    tmp_mae = []\n",
    "    for NN in optimal_NNs:\n",
    "        test_loss, test_mae, test_mse = NN.evaluate(sc_features[i], sc_label[i],batch_size=10,  verbose=1)\n",
    "        tmp_mae.append(test_mae)\n",
    "\n",
    "        #sc_predictions = correlation_plots(NN, sc_label[i], sc_features[i], \"Testing Correlation Plot for SC \" + str(i), \"Actual\", \"Predicted\")\n",
    "        #plotGraph(sc_label[i], sc_predictions, \"SC Plot\")\n",
    "\n",
    "    sc_mae.append(tmp_mae)\n",
    "    \n",
    "\n",
    "for i in sc_mae:\n",
    "    print(min(i), sum(i)/len(i) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662f4abb",
   "metadata": {},
   "source": [
    "### Isolating Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283db4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data labels with time\n",
    "time_index= []\n",
    "for i in range(0, 51):\n",
    "    time_index.append(np.where(dataset['Time'].to_numpy()  == i)[0])\n",
    "\n",
    "    \n",
    "scaled_features = scaleDataset(all_features.copy())\n",
    "#The full features of the data points that use certain time values\n",
    "time_features = []\n",
    "time_labels = []\n",
    "\n",
    "for i in range(0, 51):\n",
    "    time_features.append(scaled_features.iloc[time_index[i]])\n",
    "    #The stupid labels for each second\n",
    "    time_labels.append(data_labels.to_numpy()[time_index[i]])\n",
    "\n",
    "\n",
    "time_mae = []\n",
    "for i in range(15, 51):\n",
    "    tmp_mae = []\n",
    "    #print(\"TIME = \", i, \"S\")\n",
    "    for NN in optimal_NNs:\n",
    "        test_loss, test_mae, test_mse = NN.evaluate(time_features[i], time_labels[i], batch_size=10,  verbose=1)\n",
    "        tmp_mae.append(test_mae)\n",
    "\n",
    "        \n",
    "        #time_predictions = correlation_plots(NN, time_labels[i], time_features[i], \"Testing Correlation Plot for Time \" + str(i), \"Actual\", \"Predicted\")\n",
    "        #plotGraph(time_labels[i], time_predictions, \"Time Plot\")\n",
    "\n",
    "    time_mae.append(tmp_mae)\n",
    "\n",
    "mins = []\n",
    "averages = []\n",
    "for i in time_mae:\n",
    "    mins.append(min(i))\n",
    "    averages.append(sum(i)/len(i))\n",
    "\n",
    "for j in range(0, len(time_mae)):\n",
    "    print(mins[j], averages[j])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5739db",
   "metadata": {},
   "source": [
    "### Isolating Spin Coating and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5b6271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Spin Coating, then seperating by time\n",
    "\n",
    "ss_1 = np.where(dataset['Spin Coating'].to_numpy()  ==  1)[0]\n",
    "ss_0 = np.where(dataset['Spin Coating'].to_numpy()  ==  0)[0]\n",
    "\n",
    "times_index = []\n",
    "times_0 = []\n",
    "\n",
    "shared_time_1 = []\n",
    "shared_time_0 = []\n",
    "\n",
    "for i in range(0, 51):\n",
    "    times_index.append(np.where(dataset['Time'].to_numpy()  == i)[0].tolist())\n",
    "\n",
    "    time_1_tmp = []\n",
    "    time_0_tmp = []\n",
    "    \n",
    "    for index_sc in ss_1:\n",
    "        if index_sc in times_index[i]:\n",
    "            time_1_tmp.append(index_sc)\n",
    "        else:\n",
    "            time_0_tmp.append(index_sc)\n",
    "            \n",
    "    shared_time_1.append(time_1_tmp)\n",
    "    shared_time_0.append(time_0_tmp)\n",
    "\n",
    "scaled_features = scaleDataset(all_features.copy())\n",
    "\n",
    "shared_features = []\n",
    "shared_labels = []\n",
    "\n",
    "for i in range(0, 51):\n",
    "    shared_features.append([scaled_features.iloc[shared_time_0[i]] , scaled_features.iloc[shared_time_1[i]]])\n",
    "    shared_labels.append([data_labels.to_numpy()[shared_time_0[i]], data_labels.to_numpy()[shared_time_1[i]]])\n",
    "\n",
    "\n",
    "shared_mae = []\n",
    "for i in range(15, 51):\n",
    "    sc_tmp_mae = []\n",
    "\n",
    "    for j in range(0, 2):\n",
    "        tmp_mae = []\n",
    "        #print(\"TIME = \", i, \"S\", \"SPINCOATED = \", j)\n",
    "\n",
    "        for NN in optimal_NNs:\n",
    "            test_loss, test_mae, test_mse = NN.evaluate(shared_features[i][j], shared_labels[i][j], batch_size=10,  verbose=0)\n",
    "            tmp_mae.append(test_mae)\n",
    "\n",
    "                \n",
    "            #shared_predictions = correlation_plots(NN, shared_labels[i][j], shared_features[i][j].to_numpy(),  \"Testing Correlation Plot for Shared \" + str(j) + \" at time: \" + str(i), \"Actual\", \"Predicted\")\n",
    "            #plotGraph(shared_labels[i][j], shared_predictions, \"Shared Plot\")\n",
    "\n",
    "        sc_tmp_mae.append(tmp_mae)\n",
    "    shared_mae.append(sc_tmp_mae)\n",
    "\n",
    "mins = []\n",
    "averages = []\n",
    "for i in shared_mae:\n",
    "    mins.append([min(i[0]), min(i[1])])\n",
    "    averages.append([sum(i[0])/len(i[0]), sum(i[1])/len(i[1])])\n",
    "\n",
    "\n",
    "for j in range(0, len(shared_mae)):\n",
    "    print(mins[j], averages[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1833c4",
   "metadata": {},
   "source": [
    "### Isolating Increasing PPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93942f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data labels with spin coating 0 and 1\n",
    "\n",
    "increasing_index = [np.where(dataset['Increasing PPM'].to_numpy()  == 0)[0], np.where(dataset['Increasing PPM'].to_numpy()  == 1)[0]]\n",
    "\n",
    "scaled_features = scaleDataset(all_features.copy())\n",
    "\n",
    "#The full features of the data points that use Spin Coating\n",
    "increasing_features = [scaled_features.iloc[sc_index[0]], scaled_features.iloc[sc_index[1]]]\n",
    "\n",
    "#The stupid labels for Spin coating vs. not Spin coating\n",
    "increasing_label = [data_labels.to_numpy()[sc_index[0]], data_labels.to_numpy()[sc_index[1]]]\n",
    "\n",
    "increasing_mae = []\n",
    "for i in range(0, 2):\n",
    "    tmp_mae = []\n",
    "    for NN in optimal_NNs:\n",
    "        test_loss, test_mae, test_mse = NN.evaluate(increasing_features[i], increasing_label[i],batch_size=10,  verbose=1)\n",
    "        tmp_mae.append(test_mae)\n",
    "\n",
    "        #sc_predictions = correlation_plots(NN, sc_label[i], sc_features[i], \"Testing Correlation Plot for SC \" + str(i), \"Actual\", \"Predicted\")\n",
    "        #plotGraph(sc_label[i], sc_predictions, \"SC Plot\")\n",
    "\n",
    "    increasing_mae.append(tmp_mae)\n",
    "    \n",
    "\n",
    "for i in increasing_mae:\n",
    "    print(min(i), sum(i)/len(i) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea527b3",
   "metadata": {},
   "source": [
    "### Repeat Sensor Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cb0241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data labels with RSU\n",
    "repeat_index= []\n",
    "for i in range(1, 4):\n",
    "    repeat_index.append(np.where(dataset['Repeat Sensor Use'].to_numpy()  == i)[0])\n",
    "\n",
    "shared_tr_1 = []\n",
    "shared_tr_2 = []\n",
    "shared_tr_3 = []\n",
    "\n",
    "times_index = []\n",
    "for i in range(0, 51):\n",
    "    times_index.append(np.where(dataset['Time'].to_numpy()  == i)[0].tolist())\n",
    "\n",
    "    tr_1_tmp = []\n",
    "    tr_2_tmp = []\n",
    "    tr_3_tmp = []\n",
    "\n",
    "    for j in range(len(repeat_index)):\n",
    "   \n",
    "        for index_123 in repeat_index[j]:\n",
    "\n",
    "            if index_123 in times_index[i] and j == 0:\n",
    "                tr_1_tmp.append(index_123)\n",
    "            elif index_123 in times_index[i] and j == 1:\n",
    "                tr_2_tmp.append(index_123)\n",
    "            elif index_123 in times_index[i] and j == 2:\n",
    "                tr_3_tmp.append(index_123)\n",
    "\n",
    "#            time_0_tmp.append(index_sc)\n",
    "        \n",
    "\n",
    "    shared_tr_1.append(tr_1_tmp)\n",
    "    shared_tr_2.append(tr_2_tmp)\n",
    "    shared_tr_3.append(tr_3_tmp)\n",
    "\n",
    "scaled_features = scaleDataset(all_features.copy())\n",
    "#The full features of the data points that use certain time values\n",
    "tr_features = []\n",
    "tr_labels = []\n",
    "\n",
    "\n",
    "for i in range(0, 51):\n",
    "    tr_features.append([\n",
    "        scaled_features.iloc[shared_tr_1[i]], \n",
    "        scaled_features.iloc[shared_tr_2[i]], \n",
    "        scaled_features.iloc[shared_tr_3[i]]\n",
    "        ])\n",
    "\n",
    "    tr_labels.append([\n",
    "        data_labels.to_numpy()[shared_tr_1[i]], \n",
    "        data_labels.to_numpy()[shared_tr_2[i]], \n",
    "        data_labels.to_numpy()[shared_tr_3[i]]\n",
    "        ])\n",
    "\n",
    "tr_mae = []\n",
    "for i in range(15, 51):\n",
    "    tr_tmp_mae = []\n",
    "\n",
    "    for j in range(0, 3):\n",
    "\n",
    "        tmp_mae = []\n",
    "        \n",
    "        for NN in optimal_NNs:\n",
    "            test_loss, test_mae, test_mse = NN.evaluate(tr_features[i][j], tr_labels[i][j], batch_size=2,  verbose=0)\n",
    "            tmp_mae.append(test_mae)\n",
    "\n",
    "            #repeat_predictions = correlation_plots(NN, repeat_labels[i], repeat_features[i], \"Testing Correlation Plot for RSU \" + str(i), \"Actual\", \"Predicted\")\n",
    "            #plotGraph(RSU_labels[i], RSU_predictions, \"RSU Plot\")\n",
    "\n",
    "        tr_tmp_mae.append(tmp_mae)\n",
    "    tr_mae.append(tr_tmp_mae)\n",
    "\n",
    "mins = []\n",
    "averages = []\n",
    "for i in tr_mae:\n",
    "    mins.append([min(i[0]), min(i[1]), min(i[2])])\n",
    "    averages.append([sum(i[0])/len(i[0]), sum(i[1])/len(i[1]), sum(i[2])/len(i[2])])\n",
    "\n",
    "for j in range(0, len(tr_mae)):\n",
    "    print(mins[j], averages[j])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ff542ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2413 - mae: 0.3310 - mse: 0.2413\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2279 - mae: 0.2974 - mse: 0.2279\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2098 - mae: 0.3359 - mse: 0.2098\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2918 - mae: 0.4398 - mse: 0.2732\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.3038 - mae: 0.3779 - mse: 0.2840\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.3567 - mae: 0.5086 - mse: 0.3295\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3398 - mae: 0.4802 - mse: 0.2995\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1947 - mae: 0.2629 - mse: 0.1471\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9456 - mae: 0.7466 - mse: 0.8198\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2259 - mae: 0.3163 - mse: 0.2259\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2088 - mae: 0.2854 - mse: 0.2088\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1993 - mae: 0.3250 - mse: 0.1993\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.2937 - mae: 0.4137 - mse: 0.2738\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.3050 - mae: 0.3996 - mse: 0.3258\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4108 - mae: 0.5745 - mse: 0.4060\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3521 - mae: 0.6585 - mse: 0.5868\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1799 - mae: 0.3658 - mse: 0.2103\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.9254 - mae: 0.9333 - mse: 1.2279\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.2100 - mae: 0.3068 - mse: 0.2100\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1948 - mae: 0.2804 - mse: 0.1948\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1940 - mae: 0.3214 - mse: 0.1940\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2719 - mae: 0.4308 - mse: 0.2831\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.3037 - mae: 0.3935 - mse: 0.3047\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4367 - mae: 0.5889 - mse: 0.4231\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3670 - mae: 0.4411 - mse: 0.2888\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1755 - mae: 0.2565 - mse: 0.1336\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8172 - mae: 0.6221 - mse: 0.6508\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1943 - mae: 0.2945 - mse: 0.1943\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1833 - mae: 0.2732 - mse: 0.1833\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1863 - mae: 0.3156 - mse: 0.1863\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.2708 - mae: 0.4392 - mse: 0.2805\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.2847 - mae: 0.3811 - mse: 0.2873\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.4258 - mae: 0.5842 - mse: 0.4154\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3324 - mae: 0.5977 - mse: 0.4590\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1619 - mae: 0.3378 - mse: 0.1811\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6433 - mae: 0.7486 - mse: 0.7819\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1790 - mae: 0.2809 - mse: 0.1790\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1699 - mae: 0.2591 - mse: 0.1699\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1852 - mae: 0.3108 - mse: 0.1852\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2793 - mae: 0.4418 - mse: 0.2838\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2697 - mae: 0.3342 - mse: 0.2437\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.4346 - mae: 0.6112 - mse: 0.4408\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3110 - mae: 0.5057 - mse: 0.3201\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1565 - mae: 0.2140 - mse: 0.1151\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5665 - mae: 0.5256 - mse: 0.4444\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1693 - mae: 0.2795 - mse: 0.1693\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1602 - mae: 0.2474 - mse: 0.1602\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1764 - mae: 0.2949 - mse: 0.1764\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2974 - mae: 0.4358 - mse: 0.2938\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.2726 - mae: 0.3475 - mse: 0.2597\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.4133 - mae: 0.5748 - mse: 0.4023\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3037 - mae: 0.6417 - mse: 0.5437\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1577 - mae: 0.3814 - mse: 0.2265\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5205 - mae: 0.8278 - mse: 0.9960\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1595 - mae: 0.2768 - mse: 0.1595\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1542 - mae: 0.2426 - mse: 0.1542\n",
      "44/44 [==============================] - 0s 3ms/step - loss: 0.1658 - mae: 0.2854 - mse: 0.1658\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.2628 - mae: 0.4165 - mse: 0.2685\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.2549 - mae: 0.3501 - mse: 0.2516\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.3774 - mae: 0.5582 - mse: 0.3817\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3001 - mae: 0.4753 - mse: 0.2766\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1544 - mae: 0.2530 - mse: 0.1204\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4933 - mae: 0.5640 - mse: 0.4430\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1589 - mae: 0.2790 - mse: 0.1589\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1600 - mae: 0.2517 - mse: 0.1600\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1610 - mae: 0.2800 - mse: 0.1610\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2222 - mae: 0.3734 - mse: 0.2113\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.2418 - mae: 0.3547 - mse: 0.2298\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.3282 - mae: 0.4931 - mse: 0.3101\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2819 - mae: 0.4804 - mse: 0.2765\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1517 - mae: 0.2122 - mse: 0.1112\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4700 - mae: 0.4679 - mse: 0.3563\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1582 - mae: 0.2766 - mse: 0.1582\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1706 - mae: 0.2618 - mse: 0.1706\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1549 - mae: 0.2719 - mse: 0.1549\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.1980 - mae: 0.3604 - mse: 0.1989\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.2296 - mae: 0.3659 - mse: 0.2176\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.3059 - mae: 0.4834 - mse: 0.3132\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2629 - mae: 0.4654 - mse: 0.2587\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1504 - mae: 0.2231 - mse: 0.1109\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4633 - mae: 0.6607 - mse: 0.5649\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1613 - mae: 0.2793 - mse: 0.1613\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1768 - mae: 0.2674 - mse: 0.1768\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1519 - mae: 0.2712 - mse: 0.1519\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.1728 - mae: 0.3457 - mse: 0.1798\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.2111 - mae: 0.3880 - mse: 0.2294\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.2977 - mae: 0.4688 - mse: 0.3086\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2590 - mae: 0.4591 - mse: 0.2535\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1517 - mae: 0.4578 - mse: 0.3252\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4634 - mae: 0.4683 - mse: 0.3479\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1655 - mae: 0.2865 - mse: 0.1655\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1799 - mae: 0.2722 - mse: 0.1799\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1499 - mae: 0.2696 - mse: 0.1499\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1497 - mae: 0.3209 - mse: 0.1567\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1998 - mae: 0.3653 - mse: 0.2106\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2820 - mae: 0.4564 - mse: 0.2935\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2514 - mae: 0.4452 - mse: 0.2422\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1474 - mae: 0.2347 - mse: 0.1090\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4502 - mae: 0.5622 - mse: 0.4044\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1679 - mae: 0.2934 - mse: 0.1679\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1813 - mae: 0.2736 - mse: 0.1813\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1484 - mae: 0.2687 - mse: 0.1484\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.1297 - mae: 0.3062 - mse: 0.1401\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1989 - mae: 0.3286 - mse: 0.1814\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2660 - mae: 0.4567 - mse: 0.2928\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2433 - mae: 0.3793 - mse: 0.1965\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1411 - mae: 0.2753 - mse: 0.1161\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4427 - mae: 0.6340 - mse: 0.4905\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1715 - mae: 0.2969 - mse: 0.1715\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1808 - mae: 0.2732 - mse: 0.1808\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1454 - mae: 0.2662 - mse: 0.1454\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1122 - mae: 0.2574 - mse: 0.1066\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.2012 - mae: 0.3354 - mse: 0.1912\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2539 - mae: 0.4067 - mse: 0.2486\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2500 - mae: 0.5760 - mse: 0.4587\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1420 - mae: 0.3899 - mse: 0.2123\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4342 - mae: 0.7529 - mse: 0.7257\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1760 - mae: 0.3026 - mse: 0.1760\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1816 - mae: 0.2740 - mse: 0.1816\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1438 - mae: 0.2650 - mse: 0.1438\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1055 - mae: 0.2601 - mse: 0.1070\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1978 - mae: 0.3316 - mse: 0.1986\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.2461 - mae: 0.4203 - mse: 0.2564\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2496 - mae: 0.4919 - mse: 0.3175\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1402 - mae: 0.3019 - mse: 0.1304\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4295 - mae: 0.6966 - mse: 0.5938\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1796 - mae: 0.3043 - mse: 0.1796\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1846 - mae: 0.2761 - mse: 0.1846\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1458 - mae: 0.2673 - mse: 0.1458\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0975 - mae: 0.2390 - mse: 0.0957\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.1706 - mae: 0.3024 - mse: 0.1594\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2404 - mae: 0.4108 - mse: 0.2504\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2522 - mae: 0.4582 - mse: 0.2773\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1398 - mae: 0.3678 - mse: 0.1832\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4337 - mae: 0.5824 - mse: 0.4101\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1827 - mae: 0.3059 - mse: 0.1827\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1856 - mae: 0.2772 - mse: 0.1856\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1478 - mae: 0.2678 - mse: 0.1478\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0900 - mae: 0.2184 - mse: 0.0853\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.1373 - mae: 0.2996 - mse: 0.1442\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.2325 - mae: 0.3818 - mse: 0.2219\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2338 - mae: 0.3842 - mse: 0.2022\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1322 - mae: 0.2324 - mse: 0.0976\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4236 - mae: 0.5035 - mse: 0.3375\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1841 - mae: 0.3087 - mse: 0.1841\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1831 - mae: 0.2739 - mse: 0.1831\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1480 - mae: 0.2691 - mse: 0.1480\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0850 - mae: 0.2107 - mse: 0.0805\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1125 - mae: 0.2843 - mse: 0.1209\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.2158 - mae: 0.3610 - mse: 0.2054\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2107 - mae: 0.3315 - mse: 0.1633\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1297 - mae: 0.2725 - mse: 0.1055\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4269 - mae: 0.5705 - mse: 0.4011\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1886 - mae: 0.3116 - mse: 0.1886\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1846 - mae: 0.2736 - mse: 0.1846\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1537 - mae: 0.2759 - mse: 0.1537\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0774 - mae: 0.2003 - mse: 0.0777\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0889 - mae: 0.2453 - mse: 0.0841\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1915 - mae: 0.3478 - mse: 0.1939\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2024 - mae: 0.3658 - mse: 0.1758\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1280 - mae: 0.3714 - mse: 0.1836\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4497 - mae: 0.4449 - mse: 0.3310\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1904 - mae: 0.3133 - mse: 0.1904\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1834 - mae: 0.2718 - mse: 0.1834\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1553 - mae: 0.2784 - mse: 0.1553\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0744 - mae: 0.1987 - mse: 0.0745\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0760 - mae: 0.2456 - mse: 0.0754\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1835 - mae: 0.3270 - mse: 0.1797\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1886 - mae: 0.3937 - mse: 0.1912\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1120 - mae: 0.2461 - mse: 0.0890\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4525 - mae: 0.7080 - mse: 0.6242\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1924 - mae: 0.3156 - mse: 0.1924\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1799 - mae: 0.2678 - mse: 0.1799\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1526 - mae: 0.2739 - mse: 0.1526\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0721 - mae: 0.1862 - mse: 0.0661\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0669 - mae: 0.2383 - mse: 0.0657\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1773 - mae: 0.3263 - mse: 0.1716\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1779 - mae: 0.3738 - mse: 0.1729\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0996 - mae: 0.2240 - mse: 0.0777\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4416 - mae: 0.6802 - mse: 0.5726\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1902 - mae: 0.3157 - mse: 0.1902\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1759 - mae: 0.2651 - mse: 0.1759\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1502 - mae: 0.2698 - mse: 0.1502\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0716 - mae: 0.2114 - mse: 0.0791\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0594 - mae: 0.2260 - mse: 0.0581\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1723 - mae: 0.3368 - mse: 0.1831\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1597 - mae: 0.3933 - mse: 0.1899\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0810 - mae: 0.2701 - mse: 0.0986\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4194 - mae: 0.5443 - mse: 0.3743\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1912 - mae: 0.3187 - mse: 0.1912\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1731 - mae: 0.2636 - mse: 0.1731\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1515 - mae: 0.2711 - mse: 0.1515\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0678 - mae: 0.2006 - mse: 0.0737\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0490 - mae: 0.1962 - mse: 0.0461\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1621 - mae: 0.3355 - mse: 0.1755\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1551 - mae: 0.3306 - mse: 0.1403\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0732 - mae: 0.1711 - mse: 0.0547\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3988 - mae: 0.5107 - mse: 0.3388\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1907 - mae: 0.3187 - mse: 0.1907\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1719 - mae: 0.2634 - mse: 0.1719\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1522 - mae: 0.2712 - mse: 0.1522\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0658 - mae: 0.1968 - mse: 0.0680\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0416 - mae: 0.1766 - mse: 0.0423\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1329 - mae: 0.2946 - mse: 0.1340\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1513 - mae: 0.2695 - mse: 0.1142\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0619 - mae: 0.2631 - mse: 0.1009\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.3663 - mae: 0.4575 - mse: 0.2876\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1915 - mae: 0.3213 - mse: 0.1915\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1707 - mae: 0.2659 - mse: 0.1707\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1538 - mae: 0.2738 - mse: 0.1538\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0651 - mae: 0.1964 - mse: 0.0713\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0388 - mae: 0.1491 - mse: 0.0360\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1334 - mae: 0.2860 - mse: 0.1399\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1478 - mae: 0.2356 - mse: 0.1084\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0533 - mae: 0.1445 - mse: 0.0404\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3220 - mae: 0.3888 - mse: 0.2378\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1954 - mae: 0.3251 - mse: 0.1954\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1770 - mae: 0.2711 - mse: 0.1770\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1557 - mae: 0.2755 - mse: 0.1557\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0656 - mae: 0.1796 - mse: 0.0649\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0387 - mae: 0.1537 - mse: 0.0375\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1433 - mae: 0.2915 - mse: 0.1497\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1442 - mae: 0.2824 - mse: 0.1157\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0423 - mae: 0.1375 - mse: 0.0327\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2901 - mae: 0.4327 - mse: 0.2357\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1974 - mae: 0.3260 - mse: 0.1974\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1826 - mae: 0.2785 - mse: 0.1826\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1575 - mae: 0.2767 - mse: 0.1575\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0640 - mae: 0.1676 - mse: 0.0592\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0432 - mae: 0.1773 - mse: 0.0471\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.1498 - mae: 0.2951 - mse: 0.1407\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.1463 - mae: 0.3429 - mse: 0.1503\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0383 - mae: 0.1969 - mse: 0.0545\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2678 - mae: 0.4152 - mse: 0.2169\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1955 - mae: 0.3247 - mse: 0.1955\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1841 - mae: 0.2820 - mse: 0.1841\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1558 - mae: 0.2742 - mse: 0.1558\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0631 - mae: 0.1709 - mse: 0.0628\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0449 - mae: 0.1798 - mse: 0.0481\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.1591 - mae: 0.3162 - mse: 0.1605\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1494 - mae: 0.2891 - mse: 0.1179\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0354 - mae: 0.1268 - mse: 0.0270\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2494 - mae: 0.3872 - mse: 0.1951\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1943 - mae: 0.3209 - mse: 0.1943\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1871 - mae: 0.2847 - mse: 0.1871\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1535 - mae: 0.2670 - mse: 0.1535\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0633 - mae: 0.1652 - mse: 0.0654\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0466 - mae: 0.1686 - mse: 0.0442\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.1723 - mae: 0.3370 - mse: 0.1683\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1504 - mae: 0.3398 - mse: 0.1432\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0301 - mae: 0.1072 - mse: 0.0222\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2300 - mae: 0.4712 - mse: 0.2569\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1909 - mae: 0.3136 - mse: 0.1909\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1877 - mae: 0.2852 - mse: 0.1877\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1497 - mae: 0.2618 - mse: 0.1497\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0653 - mae: 0.1761 - mse: 0.0680\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0520 - mae: 0.1734 - mse: 0.0499\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.1910 - mae: 0.3695 - mse: 0.1946\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1565 - mae: 0.3728 - mse: 0.1684\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0287 - mae: 0.1721 - mse: 0.0400\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2197 - mae: 0.4455 - mse: 0.2307\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1898 - mae: 0.3117 - mse: 0.1898\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1865 - mae: 0.2845 - mse: 0.1865\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1471 - mae: 0.2622 - mse: 0.1471\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0662 - mae: 0.1673 - mse: 0.0600\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0580 - mae: 0.1840 - mse: 0.0556\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.2067 - mae: 0.3770 - mse: 0.1996\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1660 - mae: 0.3105 - mse: 0.1311\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0314 - mae: 0.1239 - mse: 0.0245\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2165 - mae: 0.4266 - mse: 0.2154\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1887 - mae: 0.3115 - mse: 0.1887\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1863 - mae: 0.2846 - mse: 0.1863\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1481 - mae: 0.2660 - mse: 0.1481\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0698 - mae: 0.1891 - mse: 0.0735\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0628 - mae: 0.1910 - mse: 0.0639\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.2255 - mae: 0.3938 - mse: 0.2150\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1760 - mae: 0.4030 - mse: 0.1946\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0355 - mae: 0.1270 - mse: 0.0267\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2243 - mae: 0.4821 - mse: 0.2890\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1865 - mae: 0.3108 - mse: 0.1865\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1826 - mae: 0.2819 - mse: 0.1826\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1504 - mae: 0.2730 - mse: 0.1504\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0723 - mae: 0.1927 - mse: 0.0714\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0678 - mae: 0.2070 - mse: 0.0739\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.2529 - mae: 0.4235 - mse: 0.2486\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1933 - mae: 0.4264 - mse: 0.2170\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0439 - mae: 0.2166 - mse: 0.0588\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2444 - mae: 0.4214 - mse: 0.2289\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1863 - mae: 0.3120 - mse: 0.1863\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1798 - mae: 0.2784 - mse: 0.1798\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1531 - mae: 0.2799 - mse: 0.1531\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0760 - mae: 0.2008 - mse: 0.0752\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0736 - mae: 0.2171 - mse: 0.0800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2776 - mae: 0.4476 - mse: 0.2723\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2058 - mae: 0.5031 - mse: 0.3130\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0524 - mae: 0.2159 - mse: 0.0559\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2591 - mae: 0.5252 - mse: 0.3681\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1862 - mae: 0.3169 - mse: 0.1862\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1734 - mae: 0.2738 - mse: 0.1734\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1584 - mae: 0.2896 - mse: 0.1584\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0806 - mae: 0.2105 - mse: 0.0849\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0798 - mae: 0.2063 - mse: 0.0783\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.3007 - mae: 0.4712 - mse: 0.2943\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2251 - mae: 0.4679 - mse: 0.2574\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0660 - mae: 0.2601 - mse: 0.0811\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2811 - mae: 0.4253 - mse: 0.2447\n",
      "44/44 [==============================] - 0s 2ms/step - loss: 0.1870 - mae: 0.3242 - mse: 0.1870\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1661 - mae: 0.2732 - mse: 0.1661\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1698 - mae: 0.3064 - mse: 0.1698\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0862 - mae: 0.2275 - mse: 0.0925\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0880 - mae: 0.2287 - mse: 0.0944\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.3252 - mae: 0.4751 - mse: 0.3091\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2485 - mae: 0.3760 - mse: 0.1918\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0744 - mae: 0.3057 - mse: 0.1186\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3108 - mae: 0.3645 - mse: 0.2297\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1906 - mae: 0.3326 - mse: 0.1906\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1623 - mae: 0.2758 - mse: 0.1623\n",
      "44/44 [==============================] - 0s 1ms/step - loss: 0.1851 - mae: 0.3257 - mse: 0.1851\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0914 - mae: 0.2146 - mse: 0.0831\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1059 - mae: 0.2259 - mse: 0.0963\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.3535 - mae: 0.5058 - mse: 0.3474\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.2791 - mae: 0.5628 - mse: 0.3807\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0933 - mae: 0.2667 - mse: 0.0880\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3478 - mae: 0.5997 - mse: 0.4605\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8ee46fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2973724603652954, 0.37791651487350464, 0.26288270950317383] [0.3214408755302429, 0.4420969287554423, 0.4965585271517436]\n",
      "[0.2853758633136749, 0.3996132016181946, 0.3658252954483032] [0.3088931043942769, 0.46263250708580017, 0.6525407632191976]\n",
      "[0.2803540825843811, 0.3935277462005615, 0.25652050971984863] [0.30284111698468524, 0.4710976183414459, 0.4399184286594391]\n",
      "[0.27316001057624817, 0.38111114501953125, 0.33783796429634094] [0.2943981885910034, 0.46818633874257404, 0.561365952094396]\n",
      "[0.25914132595062256, 0.33417898416519165, 0.21398741006851196] [0.28361045320828754, 0.462371826171875, 0.4150985876719157]\n",
      "[0.24737800657749176, 0.3475381135940552, 0.3814122676849365] [0.2739185442527135, 0.4527066151301066, 0.6169638832410177]\n",
      "[0.24264444410800934, 0.35014915466308594, 0.2530318796634674] [0.268285408616066, 0.4416150152683258, 0.4307607014973958]\n",
      "[0.2516763508319855, 0.3547285199165344, 0.2121509164571762] [0.27021826306978863, 0.40709011753400165, 0.3868235299984614]\n",
      "[0.26177436113357544, 0.3604322671890259, 0.22314168512821198] [0.27009227871894836, 0.40324796239535016, 0.44976289570331573]\n",
      "[0.26741930842399597, 0.3457186222076416, 0.4578498303890228] [0.27264130115509033, 0.4008278052012126, 0.4617816110452016]\n",
      "[0.2696082293987274, 0.32087695598602295, 0.23469750583171844] [0.2761079967021942, 0.38084067900975543, 0.4140304873387019]\n",
      "[0.2686799168586731, 0.30624154210090637, 0.27525025606155396] [0.27856356898943585, 0.36383692423502606, 0.4295276502768199]\n",
      "[0.26622191071510315, 0.25735199451446533, 0.3899381458759308] [0.278766135374705, 0.3331610957781474, 0.5729441344738007]\n",
      "[0.2649948000907898, 0.26007118821144104, 0.3018774688243866] [0.28053322434425354, 0.3373101552327474, 0.49681230386098224]\n",
      "[0.2673279941082001, 0.23904995620250702, 0.3678048551082611] [0.28258829315503436, 0.3173944006363551, 0.46947893500328064]\n",
      "[0.267835408449173, 0.2184309959411621, 0.23235873878002167] [0.2836414972941081, 0.299970140059789, 0.3733476847410202]\n",
      "[0.2690669298171997, 0.2106773853302002, 0.27249813079833984] [0.2838708162307739, 0.28533204396565753, 0.3915087878704071]\n",
      "[0.2735881507396698, 0.20026567578315735, 0.3658296763896942] [0.28702665368715924, 0.2644583384195964, 0.394046813249588]\n",
      "[0.27180612087249756, 0.19865930080413818, 0.24605531990528107] [0.28781179587046307, 0.2570763826370239, 0.44923477868239087]\n",
      "[0.26781296730041504, 0.18621324002742767, 0.22400875389575958] [0.2857718567053477, 0.25026754041512805, 0.426005686322848]\n",
      "[0.2650553584098816, 0.2113819420337677, 0.2700568735599518] [0.2835177977879842, 0.2580544451872508, 0.40253477295239765]\n",
      "[0.2635852098464966, 0.1962243765592575, 0.1711210012435913] [0.28447871406873065, 0.24413742125034332, 0.3374733328819275]\n",
      "[0.2633800208568573, 0.17657968401908875, 0.26306408643722534] [0.2844177484512329, 0.22265700002511343, 0.3300184905529022]\n",
      "[0.26585763692855835, 0.14910992980003357, 0.14447492361068726] [0.2869636118412018, 0.21049006779988608, 0.2563027838865916]\n",
      "[0.2711115777492523, 0.1536535769701004, 0.1374710351228714] [0.29056820273399353, 0.20822604497273764, 0.2842063754796982]\n",
      "[0.2767159640789032, 0.16756129264831543, 0.1968650072813034] [0.293731947739919, 0.21333415309588113, 0.3182916392882665]\n",
      "[0.27417778968811035, 0.170890212059021, 0.12682199478149414] [0.2936185300350189, 0.22230815887451172, 0.2676979998747508]\n",
      "[0.26701420545578003, 0.16523611545562744, 0.10718334466218948] [0.2908771832784017, 0.2236227591832479, 0.30605769405762356]\n",
      "[0.26177623867988586, 0.17340004444122314, 0.17210638523101807] [0.28684969743092853, 0.2396442194779714, 0.3301337659358978]\n",
      "[0.2622113525867462, 0.16725561022758484, 0.12385338544845581] [0.28613199790318805, 0.2427304486433665, 0.28697635730107623]\n",
      "[0.2659943401813507, 0.18906544148921967, 0.1270238310098648] [0.2873530983924866, 0.2579326579968135, 0.3373954047759374]\n",
      "[0.2730132043361664, 0.1926569789648056, 0.2165733426809311] [0.2885642647743225, 0.2743741571903229, 0.35477381447951]\n",
      "[0.27835506200790405, 0.20078232884407043, 0.215931236743927] [0.29006631175676983, 0.2884813944498698, 0.41474972168604535]\n",
      "[0.27375108003616333, 0.20629093050956726, 0.26009616255760193] [0.293425331513087, 0.29599109292030334, 0.3844332993030548]\n",
      "[0.2731775939464569, 0.2274755835533142, 0.3056997060775757] [0.30127403140068054, 0.3104386379321416, 0.34872229894002277]\n",
      "[0.2757736146450043, 0.21463485062122345, 0.2666769027709961] [0.3113535741964976, 0.31544600427150726, 0.47640442848205566]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fd19566",
   "metadata": {},
   "source": [
    "### Days Elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ab5df0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "860f9ea6",
   "metadata": {},
   "source": [
    "## 3D Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df82e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start 3D Plotting here\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "\n",
    "#xdata = np.sin(zd## 3D Plotsata) + 0.1 * np.random.randn(100)\n",
    "#ydata = np.cos(zdata) + 0.1 * np.random.randn(100)\n",
    "#ax.scatter3D(xdata, ydata, zdata, c=zdata, cmap='Greens');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "38d5604777b095764db3d9af04f1f504aafb6d6c4fdd878aa8a69b2b09a6ca27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
