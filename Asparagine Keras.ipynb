{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Regression Example With Boston Dataset: Standardized and Wider\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import keras\n",
    "import keras.utils\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from cgi import test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a260138",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('aggregated_data-tmp.csv')\n",
    "dataset = shuffle(dataset)\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "train_dataset = dataset.sample(frac=0.8, random_state=3)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "\n",
    "train_features = train_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "\n",
    "train_labels = train_features.pop('Concentration')\n",
    "test_labels = test_features.pop('Concentration')\n",
    "\n",
    "train_features = std_scaler.fit_transform(train_features.to_numpy())\n",
    "dict = {'Time':train_features[:, 0], 'Current':train_features[:, 1], 'Spin Coating':train_features[:, 2] ,'Increaing PPM':train_features[:, 3], 'Temperature':train_features[:, 4], 'Repeat Sensor Use':train_features[:, 5], 'Days Elapsed':train_features[:, 6]}\n",
    "train_features = pd.DataFrame(dict)\n",
    "\n",
    "test_features = std_scaler.fit_transform(test_features.to_numpy())\n",
    "dict = {'Time':test_features[:, 0], 'Current':test_features[:, 1], 'Spin Coating':test_features[:, 2] ,'Increaing PPM':test_features[:, 3], 'Temperature':test_features[:, 4], 'Repeat Sensor Use':test_features[:, 5], 'Days Elapsed':test_features[:, 6]}\n",
    "test_features = pd.DataFrame(dict)\n",
    "\n",
    "#For later use\n",
    "data_labels = dataset.pop('Concentration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d09760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#sns.pairplot(train_dataset[['Time','Current', 'Spin Coating', 'Increasing PPM', 'Temperature', 'Repeat Sensor Use', 'Days Elapsed', 'Concentration']], diag_kind='kde')\n",
    "\n",
    "#train_dataset = std_scaler.fit_transform(train_dataset.to_numpy())\n",
    "#dict = {'Time':train_dataset[:, 0],'Current':train_dataset[:, 1], 'Spin Coating':train_dataset[:, 2] ,'Increasing PPM':train_dataset[:, 3], 'Temperature':train_dataset[:, 4], 'Repeat Sensor Use':train_dataset[:, 5], 'Days Elapsed':train_dataset[:, 6], 'Concentration':train_dataset[:, 7] }\n",
    "#train_dataset = pd.DataFrame(dict)\n",
    "\n",
    "#test_dataset = std_scaler.transform(test_dataset)\n",
    "#dict2 = {'Time':test_dataset[:, 0],'Current':test_dataset[:, 1], 'Spin Coating':test_dataset[:, 2] ,'Increasing PPM':test_dataset[:, 3], 'Temperature':test_dataset[:, 4], 'Repeat Sensor Use':test_dataset[:, 5], 'Days Elapsed':test_dataset[:, 6], 'Concentration':test_dataset[:, 7] }\n",
    "#test_dataset = pd.DataFrame(dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2e36a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate through this a few dozen times\n",
    "\n",
    "def build_model(n1, n2):\n",
    "  #Experiment with different models, thicknesses, layers, activation functions; Don't limit to only 10 nodes; Measure up to 64 nodes in 2 layers\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(n1, activation=tf.nn.relu, input_shape=[len(train_features.keys())]),\n",
    "    layers.Dense(n2, activation=tf.nn.relu),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "  model.compile(loss='mse', optimizer=optimizer, metrics=['mae','mse'])\n",
    "  early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',patience=5)\n",
    "\n",
    "  return model\n",
    "\n",
    "def model_history(features, labels, model, epo, batch, vbs):\n",
    "  \n",
    "    history = model.fit(\n",
    "        features, labels,\n",
    "        epochs=epo, batch_size=batch, validation_split=0.2, verbose=vbs #, callbacks=early_stop\n",
    "    )\n",
    "\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    \n",
    "    return hist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97503096",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_loss(history):\n",
    "\n",
    "  plt.plot(history['loss'], label='loss')\n",
    "  plt.plot(history['val_loss'], label='val_loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "def correlation_plots(model, label, input_data, title, xlabel, ylabel):\n",
    "#test_loss, test_acc = model.evaluate(test_features, test_labels, verbose = 1)\n",
    "\n",
    "  test_predictions = model.predict(input_data).flatten()\n",
    "  plt.scatter(label,test_predictions)\n",
    "  plt.xlabel(xlabel)\n",
    "  plt.ylabel(ylabel)\n",
    "  plt.title(title)\n",
    "  plt.axis('equal')\n",
    "  plt.axis('square')\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "  return test_predictions\n",
    "\n",
    "\n",
    "def plotGraph(y_test, y_pred,regressorName):\n",
    "    plt.scatter(range(len(y_pred)), y_test, color='blue')\n",
    "    plt.scatter(range(len(y_pred)), y_pred, color='red')\n",
    "    plt.title(regressorName)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83bf62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KCrossValidation(i, features, labels, num_val_samples, epochs, batch, verbose, n1, n2):\n",
    "\n",
    "    print('processing fold #', i)\n",
    "    val_data = features[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = labels[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    partial_train_data = np.concatenate([features[:i * num_val_samples], features[(i + 1) * num_val_samples:]], axis=0)\n",
    "    partial_train_targets = np.concatenate([labels[:i * num_val_samples], labels[(i + 1) * num_val_samples:]],     axis=0)\n",
    "\n",
    "    model = build_model(n1, n2)\n",
    "\n",
    "    history = model_history(partial_train_data, partial_train_targets, model, epochs, batch, verbose)\n",
    "\n",
    "    test_loss, test_mae, test_mse = model.evaluate(val_data, val_targets, verbose=1)\n",
    "\n",
    "    return model, history, test_loss, test_mae, test_mse\n",
    "\n",
    "import numpy as np\n",
    "k_folds = 4\n",
    "num_val_samples = len(train_labels) // k_folds\n",
    "\n",
    "sum_nodes = 65\n",
    "num_epochs = 100\n",
    "batch_size = 100\n",
    "verbose = 0\n",
    "avg_val_scores = []\n",
    "order_of_architecture = []\n",
    "\n",
    "all_networks  = []\n",
    "all_history  = []\n",
    "mae_history = []\n",
    "\n",
    "\n",
    "\n",
    "#(TAKEN FROM DEEP LEARNING WITH PYTHON BY MANNING)\n",
    "for i in range(32, sum_nodes):\n",
    "\n",
    "    for j in range(32, sum_nodes):\n",
    "        if (i+j > sum_nodes):\n",
    "            continue\n",
    "        \n",
    "        print(\"first hidden layer\", i)\n",
    "        print(\"second hidden layer\", j)\n",
    "        k_fold_test_scores = []\n",
    "        k_models = []\n",
    "        k_history = []\n",
    "\n",
    "        k_mae_history = []\n",
    "\n",
    "        for fold in range(k_folds):\n",
    "            model, history, test_loss, test_mae, test_mse = KCrossValidation(\n",
    "                fold, \n",
    "                train_features, \n",
    "                train_labels, \n",
    "                num_val_samples, \n",
    "                num_epochs, \n",
    "                batch_size, \n",
    "                verbose, \n",
    "                j, \n",
    "                i)\n",
    "\n",
    "            plot_loss(history)\n",
    "            k_fold_test_scores.append(test_mae)\n",
    "            k_history.append(history)\n",
    "            k_models.append(model)\n",
    "            k_mae_history.append(history['val_mae'])\n",
    "\n",
    "\n",
    "        avg_val_scores.append(sum(k_fold_test_scores)/len(k_fold_test_scores))\n",
    "        all_history.append(k_history)\n",
    "        all_networks.append(k_models)\n",
    "\n",
    "        \n",
    "        mae_history.append([ np.mean([x[i] for x in k_mae_history]) for i in range(num_epochs)])\n",
    "\n",
    "\n",
    "        order_of_architecture.append([i, j])\n",
    "\n",
    "\n",
    "        test_predictions = correlation_plots(model, test_labels, test_features, \"Testing Correlation Plot\", \"Actual\", \"Predicted\")\n",
    "        plotGraph(test_labels, test_predictions, \"Testing Plot\")\n",
    "\n",
    "\n",
    "        training_predictions = correlation_plots(model, train_labels, train_features, \"Training Correlation Plot\", \"Actual\", \"Predicted\")\n",
    "        #plotGraph(train_labels, training_predictions, \"Training Plot\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a1780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor=0.5):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points\n",
    "\n",
    "# Find the model with the lowest error\n",
    "k_folds = 4\n",
    "lowest_index = avg_val_scores.index(min(avg_val_scores))\n",
    "optimal_NNs = all_networks[lowest_index]\n",
    "\n",
    "#print(mae_history)\n",
    "# Find the history of that model, and display it\n",
    "for i in range(k_folds):\n",
    "    x = all_history[lowest_index][i]['val_mae']\n",
    "\n",
    "   # print(x)\n",
    "plt.plot(range(1, len(mae_history[lowest_index][5:]) + 1), mae_history[lowest_index][5:])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()\n",
    "\n",
    "smooth_mae_history = smooth_curve(mae_history[lowest_index][5:])\n",
    "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea7f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data labels with spin coating 0 and 1\n",
    "sc_index = [np.where(dataset['Spin Coating'].to_numpy()  == 0)[0], np.where(dataset['Spin Coating'].to_numpy()  == 1)[0]]\n",
    "\n",
    "dataset = std_scaler.fit_transform(dataset.to_numpy())\n",
    "dict = {'Time':dataset[:, 0], 'Current':dataset[:, 1], 'Spin Coating':dataset[:, 2] ,'Increaing PPM':dataset[:, 3], 'Temperature':dataset[:, 4], 'Repeat Sensor Use':dataset[:, 5], 'Days Elapsed':dataset[:, 6]}\n",
    "dataset = pd.DataFrame(dict)\n",
    "\n",
    "#The full features of the data points that use Spin Coating\n",
    "sc_features = [dataset.iloc[sc_index[0]], dataset.iloc[sc_index[1]]]\n",
    "\n",
    "\n",
    "#The stupid labels for Spin coating vs. not Spin coating\n",
    "sc_label = [data_labels.to_numpy()[sc_index[0]], data_labels.to_numpy()[sc_index[1]]]\n",
    "\n",
    "sc_mae = []\n",
    "for NN in optimal_NNs:\n",
    "    tmp_mae = []\n",
    "    for i in range(0, 2):\n",
    "        test_loss, test_mae, test_mse = NN.evaluate(sc_features[i], sc_label[i],batch_size=10,  verbose=1)\n",
    "        tmp_mae.append(test_mae)\n",
    "\n",
    "        sc_predictions = correlation_plots(NN, sc_label[i], sc_features[i], \"Testing Correlation Plot for SC \" + str(i), \"Actual\", \"Predicted\")\n",
    "        plotGraph(sc_label[i], sc_predictions, \"SC Plot\")\n",
    "\n",
    "    sc_mae.append(tmp_mae)\n",
    "    \n",
    "\n",
    "\n",
    "print(tmp_mae)\n",
    "#train_labels = train_features.pop('Concentration')\n",
    "#test_labels = test_features.pop('Concentration')\n",
    "\n",
    "# Split the data labels according to time\n",
    "# CREATE 3D MODELS OF THE VARIOUS PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283db4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data labels with time\n",
    "\n",
    "time_index= []\n",
    "for i in range(0, 51):\n",
    "    time_index.append[np.where(dataset['Time'].to_numpy()  == i)[0]]\n",
    "\n",
    "dataset = std_scaler.fit_transform(dataset.to_numpy())\n",
    "dict = {'Time':dataset[:, 0], 'Current':dataset[:, 1], 'Spin Coating':dataset[:, 2] ,'Increaing PPM':dataset[:, 3], 'Temperature':dataset[:, 4], 'Repeat Sensor Use':dataset[:, 5], 'Days Elapsed':dataset[:, 6]}\n",
    "dataset = pd.DataFrame(dict)\n",
    "\n",
    "#The full features of the data points that use Spin Coating\n",
    "sc_features = [dataset.iloc[sc_index[0]], dataset.iloc[sc_index[1]]]\n",
    "\n",
    "\n",
    "#The stupid labels for Spin coating vs. not Spin coating\n",
    "sc_label = [data_labels.to_numpy()[sc_index[0]], data_labels.to_numpy()[sc_index[1]]]\n",
    "\n",
    "sc_mae = []\n",
    "for NN in optimal_NNs:\n",
    "    tmp_mae = []\n",
    "    for i in range(0, 2):\n",
    "        test_loss, test_mae, test_mse = NN.evaluate(sc_features[i], sc_label[i],batch_size=10,  verbose=1)\n",
    "        tmp_mae.append(test_mae)\n",
    "\n",
    "        sc_predictions = correlation_plots(NN, sc_label[i], sc_features[i], \"Testing Correlation Plot for SC \" + str(i), \"Actual\", \"Predicted\")\n",
    "        plotGraph(sc_label[i], sc_predictions, \"SC Plot\")\n",
    "\n",
    "    sc_mae.append(tmp_mae)\n",
    "    \n",
    "\n",
    "\n",
    "print(tmp_mae)\n",
    "#train_labels = train_features.pop('Concentration')\n",
    "#test_labels = test_features.pop('Concentration')\n",
    "\n",
    "# Split the data labels according to time\n",
    "# CREATE 3D MODELS OF THE VARIOUS PARAMETERS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "38d5604777b095764db3d9af04f1f504aafb6d6c4fdd878aa8a69b2b09a6ca27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
