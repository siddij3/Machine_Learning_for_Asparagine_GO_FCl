{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Regression Example With Boston Dataset: Standardized and Wider\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import keras\n",
    "import keras.utils\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from cgi import test\n",
    "\n",
    "dataset = pd.read_csv('aggregated_data.csv')\n",
    "dataset = shuffle(dataset)\n",
    "\n",
    "std_scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4948006c",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a260138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importData(data, scaler):\n",
    "\n",
    "    train_dataset = data.sample(frac=0.8, random_state=3)\n",
    "    test_dataset = data.drop(train_dataset.index)\n",
    "\n",
    "\n",
    "    train_features = train_dataset.copy()\n",
    "    test_features = test_dataset.copy()\n",
    "\n",
    "\n",
    "    train_labels = train_features.pop('Concentration')\n",
    "    test_labels = test_features.pop('Concentration')\n",
    "\n",
    "    train_features = scaler.fit_transform(train_features.to_numpy())\n",
    "    dict = {'Time':train_features[:, 0], 'Current':train_features[:, 1], 'Spin Coating':train_features[:, 2] ,'Increaing PPM':train_features[:, 3], 'Temperature':train_features[:, 4], 'Repeat Sensor Use':train_features[:, 5], 'Days Elapsed':train_features[:, 6]}\n",
    "    train_features = pd.DataFrame(dict)\n",
    "\n",
    "    test_features = scaler.fit_transform(test_features.to_numpy())\n",
    "    dict = {'Time':test_features[:, 0], 'Current':test_features[:, 1], 'Spin Coating':test_features[:, 2] ,'Increaing PPM':test_features[:, 3], 'Temperature':test_features[:, 4], 'Repeat Sensor Use':test_features[:, 5], 'Days Elapsed':test_features[:, 6]}\n",
    "    test_features = pd.DataFrame(dict)\n",
    "\n",
    "    #For later use\n",
    "    data_labels = data.pop('Concentration')\n",
    "\n",
    "    return data, train_features, test_features, train_labels, test_labels, data_labels\n",
    "#sns.pairplot(train_dataset[['Time','Current', 'Spin Coating', 'Increasing PPM', 'Temperature', 'Repeat Sensor Use', 'Days Elapsed', 'Concentration']], diag_kind='kde')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ec3bb",
   "metadata": {},
   "source": [
    "### Functions: Build NN Model, Fit Model, Plotting Loss, Correlation Plots, Plotting Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2e36a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate through this a few dozen times\n",
    "\n",
    "def build_model(n1, n2, train_feats):\n",
    "  #Experiment with different models, thicknesses, layers, activation functions; Don't limit to only 10 nodes; Measure up to 64 nodes in 2 layers\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(n1, activation=tf.nn.relu, input_shape=[len(train_feats.keys())]),\n",
    "    layers.Dense(n2, activation=tf.nn.relu),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "  model.compile(loss='mse', optimizer=optimizer, metrics=['mae','mse'])\n",
    "  early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',patience=5)\n",
    "\n",
    "  return model\n",
    "\n",
    "def model_history(features, labels, model, epo, batch, vbs):\n",
    "  \n",
    "    history = model.fit(\n",
    "        features, labels,\n",
    "        epochs=epo, batch_size=batch, validation_split=0.2, verbose=vbs #, callbacks=early_stop\n",
    "    )\n",
    "\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    \n",
    "    return hist\n",
    "\n",
    "def plot_loss(history):\n",
    "\n",
    "  plt.plot(history['loss'], label='loss')\n",
    "  plt.plot(history['val_loss'], label='val_loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "def correlation_plots(model, label, input_data, title, xlabel, ylabel):\n",
    "#test_loss, test_acc = model.evaluate(test_features, test_labels, verbose = 1)\n",
    "\n",
    "  test_predictions = model.predict(input_data).flatten()\n",
    "  plt.scatter(label,test_predictions)\n",
    "  plt.xlabel(xlabel)\n",
    "  plt.ylabel(ylabel)\n",
    "  plt.title(title)\n",
    "  plt.axis('equal')\n",
    "  plt.axis('square')\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "  return test_predictions\n",
    "\n",
    "\n",
    "def plotGraph(y_test, y_pred,regressorName):\n",
    "    plt.scatter(range(len(y_pred)), y_test, color='blue')\n",
    "    plt.scatter(range(len(y_pred)), y_pred, color='red')\n",
    "    plt.title(regressorName)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c253350f",
   "metadata": {},
   "source": [
    "#### Plotting Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97503096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "288cdd69",
   "metadata": {},
   "source": [
    "# Neural Network Creation and Selection Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db1ec8d",
   "metadata": {},
   "source": [
    "### K Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a67997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KCrossValidation(i, features, labels, num_val_samples, epochs, batch, verbose, n1, n2):\n",
    "\n",
    "    print('processing fold #', i)\n",
    "    val_data = features[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = labels[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    partial_train_data = np.concatenate([features[:i * num_val_samples], features[(i + 1) * num_val_samples:]], axis=0)\n",
    "    partial_train_targets = np.concatenate([labels[:i * num_val_samples], labels[(i + 1) * num_val_samples:]],     axis=0)\n",
    "\n",
    "    model = build_model(n1, n2, features)\n",
    "\n",
    "    history = model_history(partial_train_data, partial_train_targets, model, epochs, batch, verbose)\n",
    "\n",
    "    test_loss, test_mae, test_mse = model.evaluate(val_data, val_targets, verbose=1)\n",
    "\n",
    "    return model, history, test_loss, test_mae, test_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80992c7e",
   "metadata": {},
   "source": [
    "## NEURAL NETWORK PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88108858",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features, train_features, test_features, train_labels, test_labels, data_labels = importData(dataset.copy(), std_scaler)\n",
    "\n",
    "k_folds = 3\n",
    "num_val_samples = len(train_labels) // k_folds\n",
    "\n",
    "n1_start = 12\n",
    "n2_start = 11\n",
    "sum_nodes = 24\n",
    "\n",
    "num_epochs = 200\n",
    "batch_size = 50\n",
    "verbose = 0\n",
    "avg_val_scores = []\n",
    "order_of_architecture = []\n",
    "\n",
    "all_networks  = []\n",
    "all_history  = []\n",
    "mae_history = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5bcb0c",
   "metadata": {},
   "source": [
    "#### Where the Magic Happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83bf62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#(TAKEN FROM DEEP LEARNING WITH PYTHON BY MANNING)\n",
    "for i in range(n1_start, sum_nodes):\n",
    "\n",
    "    for j in range(n2_start, sum_nodes):\n",
    "        if (i+j > sum_nodes):\n",
    "            continue\n",
    "        \n",
    "        print(\"first hidden layer\", j)\n",
    "        print(\"second hidden layer\", i)\n",
    "        k_fold_test_scores = []\n",
    "        k_models = []\n",
    "        k_history = []\n",
    "\n",
    "        k_mae_history = []\n",
    "\n",
    "        for fold in range(k_folds):\n",
    "            model, history, test_loss, test_mae, test_mse = KCrossValidation(\n",
    "                fold, \n",
    "                train_features, \n",
    "                train_labels, \n",
    "                num_val_samples, \n",
    "                num_epochs, \n",
    "                batch_size, \n",
    "                verbose, \n",
    "                j, \n",
    "                i)\n",
    "\n",
    "            #plot_loss(history)\n",
    "            k_fold_test_scores.append(test_mae)\n",
    "            k_history.append(history)\n",
    "            k_models.append(model)\n",
    "            k_mae_history.append(history['val_mae'])\n",
    "\n",
    "\n",
    "        avg_val_scores.append(sum(k_fold_test_scores)/len(k_fold_test_scores))\n",
    "        all_history.append(k_history)\n",
    "        all_networks.append(k_models)\n",
    "\n",
    "        \n",
    "        mae_history.append([ np.mean([x[i] for x in k_mae_history]) for i in range(num_epochs)])\n",
    "\n",
    "\n",
    "        order_of_architecture.append([i, j])\n",
    "\n",
    "\n",
    "        test_predictions = correlation_plots(model, test_labels, test_features, \"Testing Correlation Plot\", \"Actual\", \"Predicted\")\n",
    "        plotGraph(test_labels, test_predictions, \"Testing Plot\")\n",
    "\n",
    "\n",
    "        #training_predictions = correlation_plots(model, train_labels, train_features, \"Training Correlation Plot\", \"Actual\", \"Predicted\")\n",
    "        #plotGraph(train_labels, training_predictions, \"Training Plot\")\n",
    "\n",
    "# Find the model with the lowest error\n",
    "lowest_index = avg_val_scores.index(min(avg_val_scores))\n",
    "optimal_NNs = all_networks[lowest_index]\n",
    "\n",
    "#print(mae_history)\n",
    "# Find the history of that model, and display it\n",
    "for i in range(k_folds):\n",
    "    x = all_history[lowest_index][i]['val_mae']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fb47e2",
   "metadata": {},
   "source": [
    "Plotting Loss Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a1780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor=0.5):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points\n",
    "\n",
    "plt.plot(range(1, len(mae_history[lowest_index][10:]) + 1), mae_history[lowest_index][10:])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()\n",
    "\n",
    "smooth_mae_history = smooth_curve(mae_history[lowest_index][10:])\n",
    "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4877308",
   "metadata": {},
   "source": [
    "Scaling Data Set Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e1152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleDataset(data):\n",
    "    data = std_scaler.fit_transform(data.to_numpy())\n",
    "    dict = {'Time':data[:, 0], 'Current':data[:, 1], 'Spin Coating':data[:, 2] ,'Increaing PPM':data[:, 3], 'Temperature':data[:, 4], 'Repeat Sensor Use':data[:, 5], 'Days Elapsed':data[:, 6]}\n",
    "    return pd.DataFrame(dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d8e06f",
   "metadata": {},
   "source": [
    "# Isolating Parameters and Printing them Out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d19d04",
   "metadata": {},
   "source": [
    "### Isolating Spin Coating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea7f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data labels with spin coating 0 and 1\n",
    "\n",
    "sc_index = [np.where(dataset['Spin Coating'].to_numpy()  == 0)[0], np.where(dataset['Spin Coating'].to_numpy()  == 1)[0]]\n",
    "\n",
    "scaled_features = scaleDataset(all_features.copy())\n",
    "\n",
    "#The full features of the data points that use Spin Coating\n",
    "sc_features = [scaled_features.iloc[sc_index[0]], scaled_features.iloc[sc_index[1]]]\n",
    "\n",
    "#The stupid labels for Spin coating vs. not Spin coating\n",
    "sc_label = [data_labels.to_numpy()[sc_index[0]], data_labels.to_numpy()[sc_index[1]]]\n",
    "\n",
    "sc_mae = []\n",
    "for i in range(0, 2):\n",
    "    tmp_mae = []\n",
    "    for NN in optimal_NNs:\n",
    "        test_loss, test_mae, test_mse = NN.evaluate(sc_features[i], sc_label[i],batch_size=10,  verbose=1)\n",
    "        tmp_mae.append(test_mae)\n",
    "\n",
    "        #sc_predictions = correlation_plots(NN, sc_label[i], sc_features[i], \"Testing Correlation Plot for SC \" + str(i), \"Actual\", \"Predicted\")\n",
    "        #plotGraph(sc_label[i], sc_predictions, \"SC Plot\")\n",
    "\n",
    "    sc_mae.append(tmp_mae)\n",
    "    \n",
    "\n",
    "for i in sc_mae:\n",
    "    print(min(i), sum(i)/len(i) )\n",
    "\n",
    "# CREATE 3D MODELS OF THE VARIOUS PARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662f4abb",
   "metadata": {},
   "source": [
    "### Isolating Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283db4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data labels with time\n",
    "time_index= []\n",
    "for i in range(0, 51):\n",
    "    time_index.append(np.where(dataset['Time'].to_numpy()  == i)[0])\n",
    "\n",
    "    \n",
    "scaled_features = scaleDataset(all_features.copy())\n",
    "#The full features of the data points that use certain time values\n",
    "time_features = []\n",
    "time_labels = []\n",
    "\n",
    "for i in range(0, 51):\n",
    "    time_features.append(scaled_features.iloc[time_index[i]])\n",
    "    #The stupid labels for each second\n",
    "    time_labels.append(data_labels.to_numpy()[time_index[i]])\n",
    "\n",
    "\n",
    "time_mae = []\n",
    "for i in range(15, 51):\n",
    "    tmp_mae = []\n",
    "    print(\"TIME = \", i, \"S\")\n",
    "    for NN in optimal_NNs:\n",
    "        test_loss, test_mae, test_mse = NN.evaluate(time_features[i], time_labels[i], batch_size=10,  verbose=1)\n",
    "        tmp_mae.append(test_mae)\n",
    "\n",
    "        \n",
    "        #time_predictions = correlation_plots(NN, time_labels[i], time_features[i], \"Testing Correlation Plot for Time \" + str(i), \"Actual\", \"Predicted\")\n",
    "        #plotGraph(time_labels[i], time_predictions, \"Time Plot\")\n",
    "\n",
    "    time_mae.append(tmp_mae)\n",
    "\n",
    "mins = []\n",
    "averages = []\n",
    "for i in time_mae:\n",
    "    mins.append(min(i))\n",
    "    averages.append(sum(i)/len(i))\n",
    "\n",
    "for j in range(0, len(time_mae)):\n",
    "    print(mins[j], averages[j])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5739db",
   "metadata": {},
   "source": [
    "### Isolating Spin Coating and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be5b6271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME =  15 S SPINCOATED =  0\n",
      "TIME =  15 S SPINCOATED =  1\n",
      "TIME =  16 S SPINCOATED =  0\n",
      "TIME =  16 S SPINCOATED =  1\n",
      "TIME =  17 S SPINCOATED =  0\n",
      "TIME =  17 S SPINCOATED =  1\n",
      "TIME =  18 S SPINCOATED =  0\n",
      "TIME =  18 S SPINCOATED =  1\n",
      "TIME =  19 S SPINCOATED =  0\n",
      "TIME =  19 S SPINCOATED =  1\n",
      "TIME =  20 S SPINCOATED =  0\n",
      "TIME =  20 S SPINCOATED =  1\n",
      "TIME =  21 S SPINCOATED =  0\n",
      "TIME =  21 S SPINCOATED =  1\n",
      "TIME =  22 S SPINCOATED =  0\n",
      "TIME =  22 S SPINCOATED =  1\n",
      "TIME =  23 S SPINCOATED =  0\n",
      "TIME =  23 S SPINCOATED =  1\n",
      "TIME =  24 S SPINCOATED =  0\n",
      "TIME =  24 S SPINCOATED =  1\n",
      "TIME =  25 S SPINCOATED =  0\n",
      "TIME =  25 S SPINCOATED =  1\n",
      "TIME =  26 S SPINCOATED =  0\n",
      "TIME =  26 S SPINCOATED =  1\n",
      "TIME =  27 S SPINCOATED =  0\n",
      "TIME =  27 S SPINCOATED =  1\n",
      "TIME =  28 S SPINCOATED =  0\n",
      "TIME =  28 S SPINCOATED =  1\n",
      "TIME =  29 S SPINCOATED =  0\n",
      "TIME =  29 S SPINCOATED =  1\n",
      "TIME =  30 S SPINCOATED =  0\n",
      "TIME =  30 S SPINCOATED =  1\n",
      "TIME =  31 S SPINCOATED =  0\n",
      "TIME =  31 S SPINCOATED =  1\n",
      "TIME =  32 S SPINCOATED =  0\n",
      "TIME =  32 S SPINCOATED =  1\n",
      "TIME =  33 S SPINCOATED =  0\n",
      "TIME =  33 S SPINCOATED =  1\n",
      "TIME =  34 S SPINCOATED =  0\n",
      "TIME =  34 S SPINCOATED =  1\n",
      "TIME =  35 S SPINCOATED =  0\n",
      "TIME =  35 S SPINCOATED =  1\n",
      "TIME =  36 S SPINCOATED =  0\n",
      "TIME =  36 S SPINCOATED =  1\n",
      "TIME =  37 S SPINCOATED =  0\n",
      "TIME =  37 S SPINCOATED =  1\n",
      "TIME =  38 S SPINCOATED =  0\n",
      "TIME =  38 S SPINCOATED =  1\n",
      "TIME =  39 S SPINCOATED =  0\n",
      "TIME =  39 S SPINCOATED =  1\n",
      "TIME =  40 S SPINCOATED =  0\n",
      "TIME =  40 S SPINCOATED =  1\n",
      "TIME =  41 S SPINCOATED =  0\n",
      "TIME =  41 S SPINCOATED =  1\n",
      "TIME =  42 S SPINCOATED =  0\n",
      "TIME =  42 S SPINCOATED =  1\n",
      "TIME =  43 S SPINCOATED =  0\n",
      "TIME =  43 S SPINCOATED =  1\n",
      "TIME =  44 S SPINCOATED =  0\n",
      "TIME =  44 S SPINCOATED =  1\n",
      "TIME =  45 S SPINCOATED =  0\n",
      "TIME =  45 S SPINCOATED =  1\n",
      "TIME =  46 S SPINCOATED =  0\n",
      "TIME =  46 S SPINCOATED =  1\n",
      "TIME =  47 S SPINCOATED =  0\n",
      "TIME =  47 S SPINCOATED =  1\n",
      "TIME =  48 S SPINCOATED =  0\n",
      "TIME =  48 S SPINCOATED =  1\n",
      "TIME =  49 S SPINCOATED =  0\n",
      "TIME =  49 S SPINCOATED =  1\n",
      "TIME =  50 S SPINCOATED =  0\n",
      "TIME =  50 S SPINCOATED =  1\n",
      "[0.4026309847831726, 0.37487995624542236] [0.4183807571729024, 0.382649044195811]\n",
      "[0.4028174877166748, 0.36404868960380554] [0.4184715747833252, 0.37531036138534546]\n",
      "[0.40304848551750183, 0.36668315529823303] [0.41856412092844647, 0.37751320004463196]\n",
      "[0.40333741903305054, 0.3606099784374237] [0.4186325669288635, 0.3763643602530162]\n",
      "[0.403652161359787, 0.34857308864593506] [0.41872557004292804, 0.3688700596491496]\n",
      "[0.40397918224334717, 0.32565274834632874] [0.41886211435000104, 0.3569411138693492]\n",
      "[0.40429866313934326, 0.309465616941452] [0.41908689339955646, 0.3442661960919698]\n",
      "[0.40456464886665344, 0.30225062370300293] [0.4192640483379364, 0.33958157896995544]\n",
      "[0.40470656752586365, 0.28984788060188293] [0.4194006423155467, 0.32826942205429077]\n",
      "[0.40484315156936646, 0.2832734286785126] [0.41953221956888836, 0.32163005073865253]\n",
      "[0.404986172914505, 0.2790781557559967] [0.4196614623069763, 0.32070109248161316]\n",
      "[0.4050785005092621, 0.27314749360084534] [0.41977207859357196, 0.3127000033855438]\n",
      "[0.40513089299201965, 0.2689317464828491] [0.41983915368715924, 0.30836625893910724]\n",
      "[0.40512344241142273, 0.2705291509628296] [0.41986367106437683, 0.30774714549382526]\n",
      "[0.4051385521888733, 0.26851120591163635] [0.41990025838216144, 0.3048313955465953]\n",
      "[0.40513646602630615, 0.2693820893764496] [0.4198961754639943, 0.3051760991414388]\n",
      "[0.4051600396633148, 0.266880065202713] [0.4198924700419108, 0.30604974428812665]\n",
      "[0.4051722288131714, 0.2694908082485199] [0.4198884069919586, 0.30838878949483234]\n",
      "[0.40516024827957153, 0.2658049762248993] [0.41987810532251996, 0.30316994587580365]\n",
      "[0.40516388416290283, 0.2660345435142517] [0.4199036955833435, 0.30322185158729553]\n",
      "[0.40518075227737427, 0.2647130787372589] [0.4198874235153198, 0.3029145499070485]\n",
      "[0.40516364574432373, 0.2735275328159332] [0.4198845823605855, 0.30975741147994995]\n",
      "[0.4051654040813446, 0.26666879653930664] [0.4199668268362681, 0.30034057299296063]\n",
      "[0.4051787555217743, 0.27011361718177795] [0.4200041989485423, 0.3022891382376353]\n",
      "[0.40519580245018005, 0.2626664340496063] [0.4200233519077301, 0.2990664442380269]\n",
      "[0.4051700532436371, 0.26908865571022034] [0.42002053062121075, 0.299309104681015]\n",
      "[0.4051305651664734, 0.2692212164402008] [0.41997870802879333, 0.3029634356498718]\n",
      "[0.40508225560188293, 0.2672041356563568] [0.4199422399202983, 0.3011889358361562]\n",
      "[0.4050367474555969, 0.270792692899704] [0.4198507269223531, 0.3067995309829712]\n",
      "[0.4049115478992462, 0.2778252065181732] [0.41975802183151245, 0.311344842116038]\n",
      "[0.4047781527042389, 0.29177385568618774] [0.4196608364582062, 0.32472694913546246]\n",
      "[0.4046427607536316, 0.2901453375816345] [0.4195382197697957, 0.3217594822247823]\n",
      "[0.40446820855140686, 0.3064420521259308] [0.4194052219390869, 0.3331441779931386]\n",
      "[0.40426480770111084, 0.31098678708076477] [0.419241060813268, 0.3383280734221141]\n",
      "[0.4040566682815552, 0.3259091377258301] [0.41903900106747943, 0.35089032848676044]\n",
      "[0.4038180112838745, 0.3360970914363861] [0.41877052187919617, 0.36225418249766034]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ss_1 = np.where(dataset['Spin Coating'].to_numpy()  ==  1)[0]\n",
    "ss_0 = np.where(dataset['Spin Coating'].to_numpy()  ==  0)[0]\n",
    "\n",
    "times_1 = []\n",
    "times_0 = []\n",
    "\n",
    "shared_time_1 = []\n",
    "shared_time_0 = []\n",
    "\n",
    "for i in range(0, 51):\n",
    "    times_1.append(np.where(dataset['Time'].to_numpy()  == i)[0].tolist())\n",
    "    times_0.append(np.where(dataset['Time'].to_numpy()  == i)[0].tolist())\n",
    "\n",
    "    time_1_tmp = []\n",
    "    time_0_tmp = []\n",
    "    \n",
    "    for index_sc in ss_1:\n",
    "        if index_sc in times_1[i]:\n",
    "            time_1_tmp.append(index_sc)\n",
    "        else:\n",
    "            time_0_tmp.append(index_sc)\n",
    "            \n",
    "    shared_time_1.append(time_1_tmp)\n",
    "    shared_time_0.append(time_0_tmp)\n",
    "\n",
    "scaled_features = scaleDataset(all_features.copy())\n",
    "\n",
    "shared_features = []\n",
    "shared_labels = []\n",
    "\n",
    "for i in range(0, 51):\n",
    "    shared_features.append([scaled_features.iloc[shared_time_0[i]] , scaled_features.iloc[shared_time_1[i]]])\n",
    "    shared_labels.append([data_labels.to_numpy()[shared_time_0[i]], data_labels.to_numpy()[shared_time_1[i]]])\n",
    "\n",
    "\n",
    "shared_mae = []\n",
    "for i in range(15, 51):\n",
    "    sc_tmp_mae = []\n",
    "\n",
    "    for j in range(0, 2):\n",
    "        tmp_mae = []\n",
    "        print(\"TIME = \", i, \"S\", \"SPINCOATED = \", j)\n",
    "\n",
    "        for NN in optimal_NNs:\n",
    "            test_loss, test_mae, test_mse = NN.evaluate(shared_features[i][j], shared_labels[i][j], batch_size=10,  verbose=0)\n",
    "            tmp_mae.append(test_mae)\n",
    "\n",
    "                \n",
    "            #shared_predictions = correlation_plots(NN, shared_labels[i][j], shared_features[i][j].to_numpy(),  \"Testing Correlation Plot for Shared \" + str(j) + \" at time: \" + str(i), \"Actual\", \"Predicted\")\n",
    "            #plotGraph(shared_labels[i][j], shared_predictions, \"Shared Plot\")\n",
    "\n",
    "        sc_tmp_mae.append(tmp_mae)\n",
    "    shared_mae.append(sc_tmp_mae)\n",
    "\n",
    "mins = []\n",
    "averages = []\n",
    "for i in shared_mae:\n",
    "    mins.append([min(i[0]), min(i[1])])\n",
    "    averages.append([sum(i[0])/len(i[0]), sum(i[1])/len(i[1])])\n",
    "\n",
    "\n",
    "for j in range(0, len(shared_mae)):\n",
    "    print(mins[j], averages[j])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "38d5604777b095764db3d9af04f1f504aafb6d6c4fdd878aa8a69b2b09a6ca27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
