{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4948006c",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Regression Example With Boston Dataset: Standardized and Wider\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import keras\n",
    "import keras.utils\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "from cgi import test\n",
    "\n",
    "dataset = pd.read_csv('aggregated_data.csv')\n",
    "\n",
    "\n",
    "dataset = shuffle(dataset)\n",
    "std_scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a260138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importData(data, scaler):\n",
    "\n",
    "    train_dataset = data.sample(frac=0.8, random_state=9578)\n",
    "    test_dataset = data.drop(train_dataset.index)\n",
    "\n",
    "    train_features = train_dataset.copy()\n",
    "    test_features = test_dataset.copy()\n",
    "\n",
    "    train_labels = train_features.pop('Concentration')\n",
    "    test_labels = test_features.pop('Concentration')\n",
    "\n",
    "    train_features = scaler.fit_transform(train_features.to_numpy())\n",
    "    dict = {\n",
    "        'Time':train_features[:, 0], \n",
    "        'Current':train_features[:, 1], \n",
    "        'Spin Coating':train_features[:, 2] ,\n",
    "        'Increaing PPM':train_features[:, 3], \n",
    "        'Temperature':train_features[:, 4], \n",
    "        'Repeat Sensor Use':train_features[:, 5], \n",
    "        'Days Elapsed':train_features[:, 6]\n",
    "        }\n",
    "    train_features = pd.DataFrame(dict)\n",
    "\n",
    "    test_features = scaler.fit_transform(test_features.to_numpy())\n",
    "    dict = {\n",
    "        'Time':test_features[:, 0], \n",
    "        'Current':test_features[:, 1], \n",
    "        'Spin Coating':test_features[:, 2] ,\n",
    "        'Increaing PPM':test_features[:, 3], \n",
    "        'Temperature':test_features[:, 4], \n",
    "        'Repeat Sensor Use':test_features[:, 5], \n",
    "        'Days Elapsed':test_features[:, 6]\n",
    "        }\n",
    "    test_features = pd.DataFrame(dict)\n",
    "\n",
    "    #For later use\n",
    "    data_labels = data.pop('Concentration')\n",
    "\n",
    "    return data, data_labels, train_dataset, test_dataset, train_features, test_features, train_labels, test_labels, \n",
    "#sns.pairplot(train_dataset[['Time','Current', 'Spin Coating', 'Increasing PPM', 'Temperature', 'Repeat Sensor Use', 'Days Elapsed', 'Concentration']], diag_kind='kde')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288cdd69",
   "metadata": {},
   "source": [
    "# Neural Network Creation and Selection Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db1ec8d",
   "metadata": {},
   "source": [
    "### Functions: Build NN Model, Fit Model, K Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a67997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n1, n2, train_feats):\n",
    "  #Experiment with different models, thicknesses, layers, activation functions; Don't limit to only 10 nodes; Measure up to 64 nodes in 2 layers\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(n1, activation=tf.nn.relu, input_shape=[len(train_feats.keys())]),\n",
    "    layers.Dense(n2, activation=tf.nn.relu),\n",
    "    layers.Dense(n2, activation=tf.nn.relu),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "  model.compile(loss='mse', optimizer=optimizer, metrics=['mae','mse'])\n",
    "  early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',patience=5)\n",
    "\n",
    "  return model\n",
    "\n",
    "def model_history(features, labels, model, epo, batch, vbs):\n",
    "  \n",
    "    history = model.fit(\n",
    "        features, labels,\n",
    "        epochs=epo, batch_size=batch, validation_split=0.2, verbose=vbs #, callbacks=early_stop\n",
    "    )\n",
    "\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    \n",
    "    return hist\n",
    "\n",
    "def KCrossValidation(i, features, labels, num_val_samples, epochs, batch, verbose, n1, n2):\n",
    "\n",
    "    print('processing fold #', i)\n",
    "    val_data = features[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = labels[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    partial_train_data = np.concatenate([features[:i * num_val_samples], features[(i + 1) * num_val_samples:]], axis=0)\n",
    "    partial_train_targets = np.concatenate([labels[:i * num_val_samples], labels[(i + 1) * num_val_samples:]],     axis=0)\n",
    "\n",
    "    model = build_model(n1, n2, features)\n",
    "\n",
    "    history = model_history(partial_train_data, partial_train_targets, model, epochs, batch, verbose)\n",
    "\n",
    "    test_loss, test_mae, test_mse = model.evaluate(val_data, val_targets, verbose=verbose)\n",
    "\n",
    "    return model, history, test_loss, test_mae, test_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80992c7e",
   "metadata": {},
   "source": [
    "## NEURAL NETWORK PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88108858",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features, data_labels, train_dataset, test_dataset, train_features, test_features, train_labels, test_labels,  = importData(dataset.copy(), std_scaler)\n",
    "display(train_features)\n",
    "k_folds = 5\n",
    "num_val_samples = len(train_labels) // k_folds\n",
    "\n",
    "n1_start = 8\n",
    "n2_start = 8\n",
    "sum_nodes = 25\n",
    "\n",
    "num_epochs = 400\n",
    "batch_size = 100\n",
    "verbose = 0\n",
    "avg_val_scores = []\n",
    "order_of_architecture = []\n",
    "\n",
    "all_networks  = []\n",
    "all_history  = []\n",
    "mae_history = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02d5d52",
   "metadata": {},
   "source": [
    "##### Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a207366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "\n",
    "  plt.plot(history['loss'], label='loss')\n",
    "  plt.plot(history['val_loss'], label='val_loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "def correlation_plots(model, label, input_data, title, xlabel, ylabel):\n",
    "\n",
    "  test_predictions = model.predict(input_data).flatten()\n",
    "  plt.scatter(label,test_predictions)\n",
    "  plt.xlabel(xlabel)\n",
    "  plt.ylabel(ylabel)\n",
    "  plt.title(title)\n",
    "  plt.axis('equal')\n",
    "  plt.axis('square')\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "  return test_predictions\n",
    "\n",
    "\n",
    "def plotGraph(y_test, y_pred,regressorName):\n",
    "    plt.scatter(range(len(y_pred)), y_test, color='blue')\n",
    "    plt.scatter(range(len(y_pred)), y_pred, color='red')\n",
    "    plt.title(regressorName)\n",
    "    plt.show()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5bcb0c",
   "metadata": {},
   "source": [
    "#### Where the Magic Happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83bf62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(TAKEN FROM DEEP LEARNING WITH PYTHON BY MANNING)\n",
    "for i in range(n1_start, sum_nodes):\n",
    "\n",
    "    for j in range(n2_start, sum_nodes):\n",
    "        if (i+j > sum_nodes):\n",
    "            continue\n",
    "        \n",
    "        print(\"first hidden layer\", j)\n",
    "        print(\"second hidden layer\", i)\n",
    "        k_fold_test_scores = []\n",
    "        k_models = []\n",
    "        k_history = []\n",
    "\n",
    "        k_mae_history = []\n",
    "\n",
    "        for fold in range(k_folds):\n",
    "            model, history, test_loss, test_mae, test_mse = KCrossValidation(\n",
    "                fold, \n",
    "                train_features, \n",
    "                train_labels, \n",
    "                num_val_samples, \n",
    "                num_epochs, \n",
    "                batch_size, \n",
    "                verbose, \n",
    "                j, \n",
    "                i)\n",
    "\n",
    "            #plot_loss(history)\n",
    "            k_fold_test_scores.append(test_mae)\n",
    "            k_history.append(history)\n",
    "            k_models.append(model)\n",
    "            k_mae_history.append(history['val_mae'])\n",
    "\n",
    "\n",
    "        avg_val_scores.append(sum(k_fold_test_scores)/len(k_fold_test_scores))\n",
    "        all_history.append(k_history)\n",
    "        all_networks.append(k_models)\n",
    "\n",
    "        \n",
    "        mae_history.append([ np.mean([x[i] for x in k_mae_history]) for i in range(num_epochs)])\n",
    "\n",
    "\n",
    "        order_of_architecture.append([i, j])\n",
    "\n",
    "\n",
    "       #test_predictions = correlation_plots(model, test_labels, test_features, \"Testing Correlation Plot\", \"Actual\", \"Predicted\")\n",
    "        #plotGraph(test_labels, test_predictions, \"Testing Plot\")\n",
    "\n",
    "\n",
    "        #training_predictions = correlation_plots(model, train_labels, train_features, \"Training Correlation Plot\", \"Actual\", \"Predicted\")\n",
    "        #plotGraph(train_labels, training_predictions, \"Training Plot\")\n",
    "\n",
    "# Find the model with the lowest error\n",
    "lowest_index = avg_val_scores.index(min(avg_val_scores))\n",
    "optimal_NNs = all_networks[lowest_index]\n",
    "\n",
    "print(avg_val_scores)\n",
    "\n",
    "# Find the history of that model, and display it\n",
    "for i in range(k_folds):\n",
    "    x = all_history[lowest_index][i]['val_mae']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929455fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def Pearson(model, features, y_true, batch, verbose_):\n",
    "\n",
    "\n",
    "    y_pred = model.predict(\n",
    "        features,\n",
    "        batch_size=None,\n",
    "        verbose=verbose_,\n",
    "        workers=3,\n",
    "        use_multiprocessing=False,\n",
    "    )\n",
    "\n",
    "    tmp_numerator = 0\n",
    "    tmp_denominator_real = 0\n",
    "    tmp_denominator_pred = 0\n",
    "\n",
    "    for i in range(len(y_pred)):\n",
    "        tmp_numerator += (y_true[i] - sum(y_true)/len(y_true))* (y_pred[i] - sum(y_pred)/len(y_pred))\n",
    "\n",
    "        tmp_denominator_real += (y_true[i] - sum(y_true)/len(y_true))**2\n",
    "        tmp_denominator_pred += (y_pred[i] - sum(y_pred)/len(y_pred))**2\n",
    "\n",
    "    R = tmp_numerator / (math.sqrt(tmp_denominator_pred) * math.sqrt(tmp_denominator_real))\n",
    "\n",
    "    return R\n",
    "\n",
    "R_all = []\n",
    "for network in all_networks:\n",
    "    R_tmp = []\n",
    "    for folded_model in network:\n",
    "        R_tmp.append(Pearson(folded_model, train_features, train_labels.to_numpy(), batch_size, verbose ))\n",
    "        print(R_tmp)\n",
    "        \n",
    "    R_all.append(sum(R_tmp)/len(R_tmp))\n",
    "\n",
    "print(R_all)\n",
    "print(len(R_all))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "326990fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.9774459], dtype=float32), array([0.9784255], dtype=float32), array([0.9799792], dtype=float32), array([0.9730387], dtype=float32), array([0.97888184], dtype=float32)]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(R_tmp)\n",
    "lowest_R_index = R_all.index(min(R_all))\n",
    "\n",
    "print(lowest_R_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fb47e2",
   "metadata": {},
   "source": [
    "Plotting Loss Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a1780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import index\n",
    "\n",
    "\n",
    "def smooth_curve(points, factor=0.8):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points\n",
    "\n",
    "print(order_of_architecture[lowest_index])\n",
    "plt.plot(range(1, len(mae_history[lowest_index][int(num_epochs/10):]) + 1), mae_history[lowest_index][int(num_epochs/10):])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()\n",
    "\n",
    "smooth_mae_history = smooth_curve(mae_history[lowest_index][int(num_epochs/10):])\n",
    "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d8e06f",
   "metadata": {},
   "source": [
    "# Isolating Parameters and Printing them Out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4877308",
   "metadata": {},
   "source": [
    "Scaling Data Set Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e1152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleDataset(data):\n",
    "    data = std_scaler.fit_transform(data.to_numpy())\n",
    "    dict = {\n",
    "        'Time':data[:, 0], \n",
    "        'Current':data[:, 1], \n",
    "        'Spin Coating':data[:, 2] ,\n",
    "        'Increaing PPM':data[:, 3], \n",
    "        'Temperature':data[:, 4], \n",
    "        'Repeat Sensor Use':data[:, 5], \n",
    "        'Days Elapsed':data[:, 6]\n",
    "        }\n",
    "    return pd.DataFrame(dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0885f848",
   "metadata": {},
   "source": [
    "## Functions for Isolating Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381c72b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsolateBinaryTime(data, parameter, start_time, batch, vbs):\n",
    "    # Splitting Spin Coating, then seperating by time\n",
    "\n",
    "    ss_1 = np.where(data[parameter].to_numpy()  ==  1)[0]\n",
    "    ss_0 = np.where(data[parameter].to_numpy()  ==  0)[0]\n",
    "\n",
    "    times_index = []\n",
    "    times_0 = []\n",
    "\n",
    "    shared_time_1 = []\n",
    "    shared_time_0 = []\n",
    "\n",
    "    for i in range(0, 51):\n",
    "        times_index.append(np.where(data['Time'].to_numpy()  == i)[0].tolist())\n",
    "\n",
    "        time_1_tmp = []\n",
    "        time_0_tmp = []\n",
    "        \n",
    "        for index_sc in ss_1:\n",
    "            if index_sc in times_index[i]:\n",
    "                time_1_tmp.append(index_sc)\n",
    "        for index_sc in ss_0:\n",
    "            if index_sc in times_index[i]:\n",
    "                time_0_tmp.append(index_sc)\n",
    "                \n",
    "        shared_time_1.append(time_1_tmp)\n",
    "        shared_time_0.append(time_0_tmp)\n",
    "\n",
    "    scaled_features = scaleDataset(all_features.copy())\n",
    "\n",
    "    shared_features = []\n",
    "    shared_labels = []\n",
    "\n",
    "    for i in range(0, 51):\n",
    "        shared_features.append([\n",
    "            scaled_features.iloc[shared_time_0[i]], \n",
    "            scaled_features.iloc[shared_time_1[i]]\n",
    "            ])\n",
    "            \n",
    "        shared_labels.append([\n",
    "            data_labels.to_numpy()[shared_time_0[i]], \n",
    "            data_labels.to_numpy()[shared_time_1[i]]\n",
    "            ])\n",
    "\n",
    "\n",
    "    shared_mae = []\n",
    "    for i in range(start_time, 51):\n",
    "        sc_tmp_mae = []\n",
    "\n",
    "        for j in range(0, 2):\n",
    "            tmp_mae = []\n",
    "            #print(\"TIME = \", i, \"S\", \"SPINCOATED = \", j)\n",
    "\n",
    "            for NN in optimal_NNs:\n",
    "                test_loss, test_mae, test_mse = NN.evaluate(\n",
    "                    shared_features[i][j], \n",
    "                    shared_labels[i][j],\n",
    "                    batch_size=batch,  \n",
    "                    verbose=vbs\n",
    "                    )\n",
    "                tmp_mae.append(test_mae)\n",
    "\n",
    "                    \n",
    "            shared_predictions = correlation_plots(\n",
    "                NN, shared_labels[i][j], \n",
    "                shared_features[i][j].to_numpy(),  \n",
    "                \"Testing Correlation Plot for Shared \" + str(j) + \" at time: \" + str(i), \n",
    "                \"Actual\", \"Predicted\"\n",
    "                )\n",
    "\n",
    "            #plotGraph(shared_labels[i][j], shared_predictions, \"Shared Plot\")\n",
    "\n",
    "            sc_tmp_mae.append(tmp_mae)\n",
    "        shared_mae.append(sc_tmp_mae)\n",
    "\n",
    "    mins = []\n",
    "    averages = []\n",
    "    for i in shared_mae:\n",
    "        mins.append([min(i[0]), min(i[1])])\n",
    "        averages.append([sum(i[0])/len(i[0]), sum(i[1])/len(i[1])])\n",
    "\n",
    "    return mins, averages, shared_mae, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9083323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolateParam(data, parameter, start_index, end_index, NN_start, batch, verbose):\n",
    "    # Split the data labels with time\n",
    "    param_index= []\n",
    "    for i in range(start_index, end_index):\n",
    "        param_index.append(np.where(data[parameter].to_numpy()  == i)[0])\n",
    "\n",
    "    scaled_features = scaleDataset(all_features.copy())\n",
    "    #The full features of the data points that use certain time values\n",
    "    param_features = []\n",
    "    param_labels = []\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        param_features.append(scaled_features.iloc[param_index[i]])\n",
    "        #The stupid labels for each second\n",
    "        param_labels.append(data_labels.to_numpy()[param_index[i]])\n",
    "\n",
    "\n",
    "    mae = []\n",
    "    for i in range(NN_start, end_index):\n",
    "        tmp_mae = []\n",
    "        #print(\"TIME = \", i, \"S\")\n",
    "        for NN in optimal_NNs:\n",
    "            test_loss, test_mae, test_mse = NN.evaluate(\n",
    "                param_features[i], \n",
    "                param_labels[i], \n",
    "                batch_size=batch,  \n",
    "                verbose=verbose\n",
    "                )\n",
    "            tmp_mae.append(test_mae)\n",
    "            \n",
    "            _predictions = correlation_plots(\n",
    "                NN, \n",
    "                param_labels[i], \n",
    "                param_features[i], \n",
    "                \"Testing Correlation Plot for Spin Coating \" + str(i), \"Actual\", \n",
    "                \"Predicted\"\n",
    "                )\n",
    "            #plotGraph(param_labels[i], _predictions, \"Time Plot\")\n",
    "\n",
    "        mae.append(tmp_mae)\n",
    "\n",
    "    mins = []\n",
    "    averages = []\n",
    "    for i in mae:\n",
    "        mins.append(min(i))\n",
    "        averages.append(sum(i)/len(i))\n",
    "\n",
    "    return mins, averages, mae\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d19d04",
   "metadata": {},
   "source": [
    "#### Isolating Spin Coating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea7f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mins_sc, averages_sc, mae_sc = isolateParam(dataset, 'Spin Coating', 0, 2, 0, 10, 1)\n",
    "print(mins_sc)\n",
    "print(averages_sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662f4abb",
   "metadata": {},
   "source": [
    "#### Isolating Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb6eddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_start_time = 10\n",
    "mins_time, averages_time, mae_time = isolateParam(dataset, 'Time', 0, 51, 1, NN_start_time, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bef7ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_tmp = []\n",
    "for i in averages_time:\n",
    "    plt_tmp.append(i)\n",
    "    \n",
    "for i in range(len(mins_time)):\n",
    "    print(mins_time[i], averages_time[i])\n",
    "\n",
    "plt.plot(range(NN_start_time, len(averages_time) +NN_start_time), plt_tmp)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Mean Absolute Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5739db",
   "metadata": {},
   "source": [
    "#### Isolating Spin Coating and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dfc20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose_isolate = 0\n",
    "mins_sct, averages_sct, shared_sct = IsolateBinaryTime(dataset, 'Spin Coating', 15, 10, verbose_isolate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1458e20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_tmp_1 = []\n",
    "plt_tmp_0 = []\n",
    "\n",
    "for i in averages_sct:\n",
    "    plt_tmp_1.append(i[1])\n",
    "    plt_tmp_0.append(i[0])\n",
    "\n",
    "for i in range(len(mins_sct)):\n",
    "    print(mins_sct[i], averages_sct[i])\n",
    "\n",
    "plt.plot(range(15, len(averages_sct) +15), plt_tmp_1)\n",
    "plt.plot(range(15, len(averages_sct) +15), plt_tmp_0)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Mean Absolute Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1833c4",
   "metadata": {},
   "source": [
    "#### Isolating Increasing PPM and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93942f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose_isolate = 1\n",
    "start_t_increasing = 15\n",
    "mins_increasing, averages_increasing, shared_increasing = IsolateBinaryTime(dataset, 'Increasing PPM', start_t_increasing, 10, verbose_isolate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d5873",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_tmp_1 = []\n",
    "plt_tmp_0 = []\n",
    "for i in averages_increasing:\n",
    "    plt_tmp_1.append(i[1])\n",
    "    plt_tmp_0.append(i[0])\n",
    "\n",
    "plt.plot(range(start_t_increasing, len(averages_increasing) +start_t_increasing), plt_tmp_1, label=\"With Spin Coating\")\n",
    "plt.plot(range(start_t_increasing, len(averages_increasing) +start_t_increasing), plt_tmp_0, label=\"Without Spin Coating\")\n",
    "plt.legend()\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Mean Absolute Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea527b3",
   "metadata": {},
   "source": [
    "#### Repeat Sensor Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cb0241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data labels with RSU\n",
    "repeat_index= []\n",
    "for i in range(1, 4):\n",
    "    repeat_index.append(np.where(dataset['Repeat Sensor Use'].to_numpy()  == i)[0])\n",
    "\n",
    "shared_tr_1 = []\n",
    "shared_tr_2 = []\n",
    "shared_tr_3 = []\n",
    "\n",
    "times_index = []\n",
    "for i in range(0, 51):\n",
    "    times_index.append(np.where(dataset['Time'].to_numpy()  == i)[0].tolist())\n",
    "\n",
    "    tr_1_tmp = []\n",
    "    tr_2_tmp = []\n",
    "    tr_3_tmp = []\n",
    "\n",
    "    for j in range(len(repeat_index)):\n",
    "   \n",
    "        for index_123 in repeat_index[j]:\n",
    "\n",
    "            if index_123 in times_index[i] and j == 0:\n",
    "                tr_1_tmp.append(index_123)\n",
    "            elif index_123 in times_index[i] and j == 1:\n",
    "                tr_2_tmp.append(index_123)\n",
    "            elif index_123 in times_index[i] and j == 2:\n",
    "                tr_3_tmp.append(index_123)\n",
    "\n",
    "#            time_0_tmp.append(index_sc)\n",
    "        \n",
    "\n",
    "    shared_tr_1.append(tr_1_tmp)\n",
    "    shared_tr_2.append(tr_2_tmp)\n",
    "    shared_tr_3.append(tr_3_tmp)\n",
    "\n",
    "scaled_features = scaleDataset(all_features.copy())\n",
    "#The full features of the data points that use certain time values\n",
    "tr_features = []\n",
    "tr_labels = []\n",
    "\n",
    "\n",
    "for i in range(0, 51):\n",
    "    tr_features.append([\n",
    "        scaled_features.iloc[shared_tr_1[i]], \n",
    "        scaled_features.iloc[shared_tr_2[i]], \n",
    "        scaled_features.iloc[shared_tr_3[i]]\n",
    "        ])\n",
    "\n",
    "    tr_labels.append([\n",
    "        data_labels.to_numpy()[shared_tr_1[i]], \n",
    "        data_labels.to_numpy()[shared_tr_2[i]], \n",
    "        data_labels.to_numpy()[shared_tr_3[i]]\n",
    "        ])\n",
    "\n",
    "tr_mae = []\n",
    "for i in range(15, 51):\n",
    "    tr_tmp_mae = []\n",
    "\n",
    "    for j in range(0, 3):\n",
    "\n",
    "        tmp_mae = []\n",
    "        \n",
    "        for NN in optimal_NNs:\n",
    "            test_loss, test_mae, test_mse = NN.evaluate(tr_features[i][j], tr_labels[i][j], batch_size=2,  verbose=0)\n",
    "            tmp_mae.append(test_mae)\n",
    "\n",
    "            #repeat_predictions = correlation_plots(NN, repeat_labels[i], repeat_features[i], \"Testing Correlation Plot for RSU \" + str(i), \"Actual\", \"Predicted\")\n",
    "            #plotGraph(RSU_labels[i], RSU_predictions, \"RSU Plot\")\n",
    "\n",
    "        tr_tmp_mae.append(tmp_mae)\n",
    "    tr_mae.append(tr_tmp_mae)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f105b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "mins_tr = []\n",
    "averages_tr = []\n",
    "for i in tr_mae:\n",
    "    mins_tr.append([min(i[0]), min(i[1]), min(i[2])])\n",
    "    averages_tr.append([sum(i[0])/len(i[0]), sum(i[1])/len(i[1]), sum(i[2])/len(i[2])])\n",
    "\n",
    "\n",
    "\n",
    "plt_tmp_1 = []\n",
    "plt_tmp_2 = []\n",
    "plt_tmp_3 = []\n",
    "\n",
    "for i in averages_tr:\n",
    "    plt_tmp_1.append(i[0])\n",
    "    plt_tmp_2.append(i[1])\n",
    "    plt_tmp_3.append(i[2])\n",
    "\n",
    "\n",
    "#for i in range(len(tr_mae)):\n",
    "#    print(mins[i], averages[i])\n",
    "\n",
    "plt.plot(range(15, len(averages_increasing) +15), plt_tmp_1, label = \"Day 1\")\n",
    "plt.plot(range(15, len(averages_increasing) +15), plt_tmp_2, label = \"Day 2\")\n",
    "plt.plot(range(15, len(averages_increasing) +15), plt_tmp_3, label = \"Day 3\")\n",
    "\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()\n",
    "\n",
    "#for j in range(0, len(tr_mae)):\n",
    "#    print(mins[j], averages[j])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "38d5604777b095764db3d9af04f1f504aafb6d6c4fdd878aa8a69b2b09a6ca27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
