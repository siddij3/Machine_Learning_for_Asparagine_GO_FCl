{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4948006c",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Importing Data\n",
    "\n",
    "# %%\n",
    "# -*- coding: utf-8 -*-\n",
    "# Regression Example With Boston Dataset: Standardized and Wider\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.utils import shuffle\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import keras\n",
    "import keras.utils\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "from cgi import test\n",
    "\n",
    "dataset = pd.read_csv('aggregated_data.csv')\n",
    "\n",
    "\n",
    "dataset = shuffle(dataset)\n",
    "std_scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a260138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importData(data, scaler):\n",
    "\n",
    "    train_dataset = data.sample(frac=0.8, random_state=9578)\n",
    "    test_dataset = data.drop(train_dataset.index)\n",
    "\n",
    "    train_features = train_dataset.copy()\n",
    "    test_features = test_dataset.copy()\n",
    "\n",
    "    train_labels = train_features.pop('Concentration')\n",
    "    test_labels = test_features.pop('Concentration')\n",
    "\n",
    "    train_features = scaler.fit_transform(train_features.to_numpy())\n",
    "    dict = {\n",
    "        'Time':train_features[:, 0], \n",
    "        'Current':train_features[:, 1], \n",
    "        'Spin Coating':train_features[:, 2] ,\n",
    "        'Increaing PPM':train_features[:, 3], \n",
    "        'Temperature':train_features[:, 4], \n",
    "        'Repeat Sensor Use':train_features[:, 5], \n",
    "        'Days Elapsed':train_features[:, 6]\n",
    "        }\n",
    "    train_features = pd.DataFrame(dict)\n",
    "\n",
    "    test_features = scaler.fit_transform(test_features.to_numpy())\n",
    "    dict = {\n",
    "        'Time':test_features[:, 0], \n",
    "        'Current':test_features[:, 1], \n",
    "        'Spin Coating':test_features[:, 2] ,\n",
    "        'Increaing PPM':test_features[:, 3], \n",
    "        'Temperature':test_features[:, 4], \n",
    "        'Repeat Sensor Use':test_features[:, 5], \n",
    "        'Days Elapsed':test_features[:, 6]\n",
    "        }\n",
    "    test_features = pd.DataFrame(dict)\n",
    "\n",
    "    #For later use\n",
    "    data_labels = data.pop('Concentration')\n",
    "\n",
    "    return data, data_labels, train_dataset, test_dataset, train_features, test_features, train_labels, test_labels, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288cdd69",
   "metadata": {},
   "source": [
    "# Neural Network Creation and Selection Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db1ec8d",
   "metadata": {},
   "source": [
    "### Functions: Build NN Model, Fit Model, K Cross Validation, Pearson Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a67997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n1, n2, train_feats):\n",
    "  #Experiment with different models, thicknesses, layers, activation functions; Don't limit to only 10 nodes; Measure up to 64 nodes in 2 layers\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(n1, activation=tf.nn.relu, input_shape=[len(train_feats.keys())]),\n",
    "    layers.Dense(n2, activation=tf.nn.relu),\n",
    "    layers.Dense(n2, activation=tf.nn.relu),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "  model.compile(loss='mse', optimizer=optimizer, metrics=['mae','mse'])\n",
    "  early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',patience=5)\n",
    "\n",
    "  return model\n",
    "\n",
    "def model_history(features, labels, model, epo, batch, vbs):\n",
    "  \n",
    "    history = model.fit(\n",
    "        features, labels,\n",
    "        epochs=epo, batch_size=batch, validation_split=0.2, verbose=vbs #, callbacks=early_stop\n",
    "    )\n",
    "\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    \n",
    "    return hist\n",
    "\n",
    "def KCrossValidation(i, features, labels, num_val_samples, epochs, batch, verbose, n1, n2):\n",
    "\n",
    "    print('processing fold #', i)\n",
    "    val_data = features[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = labels[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    partial_train_data = np.concatenate([features[:i * num_val_samples], features[(i + 1) * num_val_samples:]], axis=0)\n",
    "    partial_train_targets = np.concatenate([labels[:i * num_val_samples], labels[(i + 1) * num_val_samples:]],     axis=0)\n",
    "\n",
    "    model = build_model(n1, n2, features)\n",
    "\n",
    "    history = model_history(partial_train_data, partial_train_targets, model, epochs, batch, verbose)\n",
    "\n",
    "    test_loss, test_mae, test_mse = model.evaluate(val_data, val_targets, verbose=verbose)\n",
    "    test_R = Pearson(model, val_data, val_targets.to_numpy(), batch, verbose )[0]\n",
    "\n",
    "\n",
    "    return model, history, test_loss, test_mae, test_mse, test_R\n",
    "\n",
    "def Pearson(model, features, y_true, batch, verbose_):\n",
    "    y_pred = model.predict(\n",
    "        features,\n",
    "        batch_size=None,\n",
    "        verbose=verbose_,\n",
    "        workers=3,\n",
    "        use_multiprocessing=False,\n",
    "    )\n",
    "\n",
    "    tmp_numerator = 0\n",
    "    tmp_denominator_real = 0\n",
    "    tmp_denominator_pred = 0\n",
    "\n",
    "    for i in range(len(y_pred)):\n",
    "        tmp_numerator += (y_true[i] - sum(y_true)/len(y_true))* (y_pred[i] - sum(y_pred)/len(y_pred))\n",
    "\n",
    "        tmp_denominator_real += (y_true[i] - sum(y_true)/len(y_true))**2\n",
    "        tmp_denominator_pred += (y_pred[i] - sum(y_pred)/len(y_pred))**2\n",
    "\n",
    "    R = tmp_numerator / (math.sqrt(tmp_denominator_pred) * math.sqrt(tmp_denominator_real))\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80992c7e",
   "metadata": {},
   "source": [
    "## NEURAL NETWORK PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88108858",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features, data_labels, train_dataset, test_dataset, train_features, test_features, train_labels, test_labels,  = importData(dataset.copy(), std_scaler)\n",
    "k_folds = 2 #4\n",
    "num_val_samples = len(train_labels) // k_folds\n",
    "\n",
    "n1_start = 10 #5\n",
    "n2_start = 10 #5\n",
    "sum_nodes = 20 #128\n",
    "\n",
    "num_epochs = 100 #400\n",
    "batch_size = 100 #100\n",
    "verbose = 0\n",
    "avg_val_scores = []\n",
    "\n",
    "order_of_architecture = []\n",
    "dict_lowest_MAE  = {}\n",
    "dict_highest_R  = {}\n",
    "\n",
    "all_networks  = []\n",
    "all_history  = []\n",
    "mae_history = []\n",
    "\n",
    "R_all = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02d5d52",
   "metadata": {},
   "source": [
    "##### Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a207366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_plots(model, label, input_data, title, xlabel, ylabel):\n",
    "\n",
    "  test_predictions = model.predict(input_data).flatten()\n",
    "  #plt.scatter(label,test_predictions)\n",
    "  #plt.plot(label, label, color='black', linestyle='solid')\n",
    "  \n",
    "  #plt.xlabel(xlabel)\n",
    "  #plt.ylabel(ylabel)\n",
    "  #plt.title(title)\n",
    "  #plt.axis('equal')\n",
    "  #plt.axis('square')\n",
    "  #plt.grid(True)\n",
    "  #plt.show()\n",
    "  return test_predictions\n",
    "\n",
    "\n",
    "def plotGraph(y_test, y_pred,regressorName):\n",
    "    #plt.scatter(range(len(y_pred)), y_test, color='blue')\n",
    "    #plt.scatter(range(len(y_pred)), y_pred, color='red')\n",
    "    #plt.title(regressorName)\n",
    "    #plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5bcb0c",
   "metadata": {},
   "source": [
    "#### Where the Magic Happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a83bf62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first hidden layer 10\n",
      "second hidden layer 10\n",
      "processing fold # 0\n",
      "processing fold # 1\n",
      "processing fold # 2\n"
     ]
    }
   ],
   "source": [
    "for i in range(n1_start, sum_nodes):\n",
    "\n",
    "    for j in range(n2_start, sum_nodes):\n",
    "        if (i+j > sum_nodes):\n",
    "            continue\n",
    "        \n",
    "        print(\"first hidden layer\", j)\n",
    "        print(\"second hidden layer\", i)\n",
    "        k_fold_test_scores = []\n",
    "        k_models = []\n",
    "        k_history = []\n",
    "\n",
    "        k_mae_history = []\n",
    "        R_tmp = []\n",
    "\n",
    "        for fold in range(k_folds):\n",
    "            #Test\n",
    "            model, history, test_loss, test_mae, test_mse, test_R = KCrossValidation(\n",
    "                fold, \n",
    "                train_features, \n",
    "                train_labels, \n",
    "                num_val_samples, \n",
    "                num_epochs, \n",
    "                batch_size, \n",
    "                verbose, \n",
    "                j, \n",
    "                i)\n",
    "\n",
    "            R_tmp.append(test_R)\n",
    "            k_fold_test_scores.append(test_mae)\n",
    "            \n",
    "            k_history.append(history)\n",
    "            k_models.append(model)\n",
    "            k_mae_history.append(history['val_mae'])\n",
    "\n",
    "\n",
    "        R_all_avg = sum(R_tmp)/len(R_tmp)\n",
    "        dict_highest_R['R: i - j'.format(i, j)] = R_all_avg\n",
    "        R_all.append(R_all_avg)\n",
    "\n",
    "        avg_MAE = sum(k_fold_test_scores)/len(k_fold_test_scores)\n",
    "        dict_lowest_MAE['R: i - j'.format(i, j)] = avg_MAE\n",
    "        avg_val_scores.append(avg_MAE)\n",
    "\n",
    "\n",
    "        all_history.append(k_history)\n",
    "        all_networks.append(k_models)\n",
    "\n",
    "        mae_history.append([ np.mean([x[i] for x in k_mae_history]) for i in range(num_epochs)])\n",
    "\n",
    "        order_of_architecture.append([i, j])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "720493d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'R_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\junai\\AppData\\Roaming\\Python\\Python39\\Scripts\\Asparagine Machine Learning\\Asparagine_Keras_Pearson.ipynb Cell 13\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/junai/AppData/Roaming/Python/Python39/Scripts/Asparagine%20Machine%20Learning/Asparagine_Keras_Pearson.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(R_all)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'R_all' is not defined"
     ]
    }
   ],
   "source": [
    "print(R_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de074a",
   "metadata": {},
   "source": [
    "#### Getting Pearson Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8882f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the model with the lowest error\n",
    "lowest_mae_index = avg_val_scores.index(min(avg_val_scores))\n",
    "optimal_NNs_mae = all_networks[lowest_mae_index]\n",
    "\n",
    "highest_R_index = R_all.index(max(R_all))\n",
    "optimal_NNs_R = all_networks[highest_R_index]\n",
    "\n",
    "# Find the history of that model, and display it\n",
    "for i in range(k_folds):\n",
    "    x = all_history[lowest_mae_index][i]['val_mae']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fb47e2",
   "metadata": {},
   "source": [
    "Plotting Loss Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a1780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import index\n",
    "\n",
    "\n",
    "def smooth_curve(points, factor=0.8):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points\n",
    "\n",
    "print(order_of_architecture[lowest_mae_index])\n",
    "print(order_of_architecture[highest_R_index])\n",
    "\n",
    "plt.plot(range(1, len(mae_history[lowest_mae_index][int(num_epochs/10):]) + 1), mae_history[lowest_mae_index][int(num_epochs/10):], label=\"Lowest MAE\")\n",
    "plt.plot(range(1, len(mae_history[highest_R_index][int(num_epochs/10):]) + 1), mae_history[highest_R_index][int(num_epochs/10):], label=\"Highest R\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()\n",
    "\n",
    "smooth_mae_history = smooth_curve(mae_history[lowest_mae_index][int(num_epochs/10):])\n",
    "smooth_R_history = smooth_curve(mae_history[highest_R_index][int(num_epochs/10):])\n",
    "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history, label=\"Lowest MAE\")\n",
    "plt.plot(range(1, len(smooth_R_history) + 1), smooth_R_history, label=\"Highest R\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d8e06f",
   "metadata": {},
   "source": [
    "# Isolating Parameters and Printing them Out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4877308",
   "metadata": {},
   "source": [
    "Scaling Data Set Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e1152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleDataset(data):\n",
    "    data = std_scaler.fit_transform(data.to_numpy())\n",
    "    dict = {\n",
    "        'Time':data[:, 0], \n",
    "        'Current':data[:, 1], \n",
    "        'Spin Coating':data[:, 2] ,\n",
    "        'Increaing PPM':data[:, 3], \n",
    "        'Temperature':data[:, 4], \n",
    "        'Repeat Sensor Use':data[:, 5], \n",
    "        'Days Elapsed':data[:, 6]\n",
    "        }\n",
    "    return pd.DataFrame(dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0885f848",
   "metadata": {},
   "source": [
    "## Functions for Isolating Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9083323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolateParam(optimal_NNs, data, parameter, start_index, end_index, NN_start, batch, verbose, mae_or_R):\n",
    "    # Split the data labels with time\n",
    "    param_index= []\n",
    "    for i in range(start_index, end_index):\n",
    "        param_index.append(np.where(data[parameter].to_numpy()  == i)[0])\n",
    "\n",
    "    scaled_features = scaleDataset(all_features.copy())\n",
    "    #The full features of the data points that use certain time values\n",
    "    param_features = []\n",
    "    param_labels = []\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        param_features.append(scaled_features.iloc[param_index[i]])\n",
    "        #The stupid labels for each second\n",
    "        param_labels.append(data_labels.to_numpy()[param_index[i]])\n",
    "\n",
    "\n",
    "    mae = []\n",
    "    R = []\n",
    "    for i in range(NN_start, end_index):\n",
    "        tmp_mae = []\n",
    "        tmp_R = []\n",
    "        \n",
    "        for NN in optimal_NNs:\n",
    "            test_loss, test_mae, test_mse = NN.evaluate(\n",
    "                param_features[i], \n",
    "                param_labels[i], \n",
    "                batch_size=batch,  \n",
    "                verbose=verbose\n",
    "                )\n",
    "\n",
    "            tmp_R.append(Pearson(NN, param_features[i], param_labels[i], batch, verbose)[0])\n",
    "            \n",
    "            tmp_mae.append(test_mae)\n",
    "            \n",
    "            _predictions = correlation_plots(\n",
    "                NN, \n",
    "                param_labels[i], \n",
    "                param_features[i], \n",
    "                \"Testing Correlation Plot for \"+  parameter + \": \" + str(i) + \" \" + mae_or_R, \n",
    "                \"Actual\", \n",
    "                \"Predicted\"\n",
    "                )\n",
    "\n",
    "            \n",
    "        R.append(tmp_R)\n",
    "        mae.append(tmp_mae)\n",
    "\n",
    "    average_R = []\n",
    "    average_mae = []\n",
    "    for i in range(len(mae)):\n",
    "        average_R.append(sum(R[i])/len(R[i]))\n",
    "        average_mae.append(sum(mae[i])/len(mae[i]))\n",
    "\n",
    "\n",
    "    return average_R, average_mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381c72b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsolateBinaryTime(optimal_NNs, data, parameter, start_time, batch, vbs, mae_or_R):\n",
    "    # Splitting Spin Coating, then seperating by time\n",
    "\n",
    "    ss_1 = np.where(data[parameter].to_numpy()  ==  1)[0]\n",
    "    ss_0 = np.where(data[parameter].to_numpy()  ==  0)[0]\n",
    "\n",
    "    times_index = []\n",
    "    times_0 = []\n",
    "\n",
    "    shared_time_1 = []\n",
    "    shared_time_0 = []\n",
    "\n",
    "    for i in range(0, 51):\n",
    "        times_index.append(np.where(data['Time'].to_numpy()  == i)[0].tolist())\n",
    "\n",
    "        time_1_tmp = []\n",
    "        time_0_tmp = []\n",
    "        \n",
    "        for index_sc in ss_1:\n",
    "            if index_sc in times_index[i]:\n",
    "                time_1_tmp.append(index_sc)\n",
    "        for index_sc in ss_0:\n",
    "            if index_sc in times_index[i]:\n",
    "                time_0_tmp.append(index_sc)\n",
    "                \n",
    "        shared_time_1.append(time_1_tmp)\n",
    "        shared_time_0.append(time_0_tmp)\n",
    "\n",
    "    scaled_features = scaleDataset(all_features.copy())\n",
    "\n",
    "    shared_features = []\n",
    "    shared_labels = []\n",
    "\n",
    "    for i in range(0, 51):\n",
    "        shared_features.append([\n",
    "            scaled_features.iloc[shared_time_0[i]], \n",
    "            scaled_features.iloc[shared_time_1[i]]\n",
    "            ])\n",
    "            \n",
    "        shared_labels.append([\n",
    "            data_labels.to_numpy()[shared_time_0[i]], \n",
    "            data_labels.to_numpy()[shared_time_1[i]]\n",
    "            ])\n",
    "    \n",
    "\n",
    "    shared_mae = []\n",
    "    shared_R = []\n",
    "    for i in range(start_time, 51):\n",
    "        sc_tmp_mae = []\n",
    "        sc_tmp_R = []\n",
    "\n",
    "        for j in range(0, 2):\n",
    "            tmp_mae = []\n",
    "            tmp_R = []\n",
    "            #print(\"TIME = \", i, \"S\", \"SPINCOATED = \", j)\n",
    "\n",
    "            for NN in optimal_NNs:\n",
    "                test_loss, test_mae, test_mse = NN.evaluate(\n",
    "                    shared_features[i][j], \n",
    "                    shared_labels[i][j],\n",
    "                    batch_size=batch,  \n",
    "                    verbose=vbs\n",
    "                    )\n",
    "            \n",
    "                tmp_R.append(Pearson(NN, shared_features[i][j], shared_labels[i][j], batch, vbs)[0])\n",
    "                tmp_mae.append(test_mae)\n",
    "\n",
    "                    \n",
    "            shared_predictions = correlation_plots(\n",
    "                NN, shared_labels[i][j], \n",
    "                shared_features[i][j].to_numpy(),  \n",
    "                \"Testing Correlation Plot for Shared \" + str(j) + \" at time: \" + str(i) + \" \" + mae_or_R, \n",
    "                \"Actual\", \"Predicted\"\n",
    "                )\n",
    "            \n",
    "\n",
    "\n",
    "            #plotGraph(shared_labels[i][j], shared_predictions, \"Shared Plot\")\n",
    "\n",
    "            sc_tmp_mae.append(tmp_mae)\n",
    "            sc_tmp_R.append(tmp_R)\n",
    "\n",
    "        shared_mae.append(sc_tmp_mae)\n",
    "        shared_R.append(sc_tmp_R)\n",
    "\n",
    "\n",
    "    averages_R = []\n",
    "    averages_mae = []\n",
    "    for i in shared_mae:\n",
    "        averages_mae.append([sum(i[0])/len(i[0]), sum(i[1])/len(i[1])])\n",
    "\n",
    "    for i in shared_R:\n",
    "        averages_R.append([sum(i[0])/len(i[0]), sum(i[1])/len(i[1])])\n",
    "\n",
    "\n",
    "    return averages_R, averages_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d19d04",
   "metadata": {},
   "source": [
    "#### Isolating Spin Coating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea7f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_of_sc_mae, mae_of_sc_mae = isolateParam(optimal_NNs_mae, dataset, 'Spin Coating', 0, 2, 0, 10, 1, \"MAE\")\n",
    "R_of_sc_R, mae_of_sc_R = isolateParam(optimal_NNs_R, dataset, 'Spin Coating', 0, 2, 0, 10, 1, \"R\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662f4abb",
   "metadata": {},
   "source": [
    "#### Isolating Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb6eddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_start_time = 20\n",
    "R_time_mae, mae_averages_time_mae = isolateParam(optimal_NNs_mae, dataset, 'Time', 0, 51, NN_start_time, 100, 1, \"MAE\")\n",
    "R_time_R, mae_averages_time_R = isolateParam(optimal_NNs_R, dataset, 'Time', 0, 51, NN_start_time, 100, 1, \"R\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bef7ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(NN_start_time, len(mae_averages_time_mae) +NN_start_time), mae_averages_time_mae, label=\"Time Isolating; Optimal MAE Network\")\n",
    "plt.plot(range(NN_start_time, len(mae_averages_time_R) +NN_start_time), mae_averages_time_R, label=\"Time Isolating; Optimal R Network\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(NN_start_time, len(R_time_mae) +NN_start_time), R_time_mae, label=\"Time Isolating; Optimal MAE Network\")\n",
    "plt.plot(range(NN_start_time, len(R_time_R) +NN_start_time), R_time_R, label=\"Time Isolating; Optimal R Network\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('R')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5739db",
   "metadata": {},
   "source": [
    "#### Isolating Spin Coating and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dfc20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose_isolate = 0\n",
    "NN_start_sc_t = 15\n",
    "R_of_sct_mae, mae_of_sct_mae = IsolateBinaryTime(optimal_NNs_mae,dataset, 'Spin Coating', NN_start_sc_t, 10, verbose_isolate, \"MAE\")\n",
    "R_of_sct_R, mae_of_sct_R = IsolateBinaryTime(optimal_NNs_R, dataset, 'Spin Coating', NN_start_sc_t, 10, verbose_isolate, \"R\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1458e20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_of_mins_sct_mae_1 = [R_of_sct_mae[i][1] for i in range(len(R_of_sct_mae))]\n",
    "R_of_mins_sct_mae_0 = [R_of_sct_mae[i][0] for i in range(len(R_of_sct_mae))]\n",
    "\n",
    "mae_of_averages_sct_mae_1 = [mae_of_sct_mae[i][1] for i in range(len(mae_of_sct_mae))]\n",
    "mae_of_averages_sct_mae_0 = [mae_of_sct_mae[i][0] for i in range(len(mae_of_sct_mae))]\n",
    "\n",
    "R_of_mins_sct_R_1 = [R_of_sct_R[i][1] for i in range(len(R_of_sct_R))]\n",
    "R_of_mins_sct_R_0 = [R_of_sct_R[i][0] for i in range(len(R_of_sct_R))]\n",
    "\n",
    "mae_of_averages_sct_R_1 = [mae_of_sct_R[i][1] for i in range(len(mae_of_sct_R))]\n",
    "mae_of_averages_sct_R_0 = [mae_of_sct_R[i][0] for i in range(len(mae_of_sct_R))]\n",
    "\n",
    "\n",
    "plt.plot(range(NN_start_sc_t, len(mae_of_averages_sct_mae_1) +NN_start_sc_t), mae_of_averages_sct_mae_1, label=\"Spin Coating; Optimal MAE Network\")\n",
    "plt.plot(range(NN_start_sc_t, len(mae_of_averages_sct_mae_0) +NN_start_sc_t), mae_of_averages_sct_mae_0, label=\"No Spin Coating; Optimal MAE Network\")\n",
    "\n",
    "plt.plot(range(NN_start_sc_t, len(mae_of_averages_sct_R_1) +NN_start_sc_t), mae_of_averages_sct_R_1, label=\"Spin Coating; Optimal R Network\")\n",
    "plt.plot(range(NN_start_sc_t, len(mae_of_averages_sct_R_0) +NN_start_sc_t), mae_of_averages_sct_R_0, label=\"No Spin Coating; Optimal R Network\")\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(NN_start_sc_t, len(R_of_mins_sct_mae_1) +NN_start_sc_t), R_of_mins_sct_mae_1, label=\"Spin Coating; Optimal MAE Network\")\n",
    "plt.plot(range(NN_start_sc_t, len(R_of_mins_sct_mae_0) +NN_start_sc_t), R_of_mins_sct_mae_0, label=\"No Spin Coating; Optimal MAE Network\")\n",
    "\n",
    "plt.plot(range(NN_start_sc_t, len(R_of_mins_sct_R_1) +NN_start_sc_t), R_of_mins_sct_R_1, label=\"Spin Coating; Optimal R Network\")\n",
    "plt.plot(range(NN_start_sc_t, len(R_of_mins_sct_R_0) +NN_start_sc_t), R_of_mins_sct_R_0, label=\"No Spin Coating; Optimal R Network\")\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('R')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1833c4",
   "metadata": {},
   "source": [
    "#### Isolating Increasing PPM and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93942f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose_isolate = 1\n",
    "start_t_increasing = 15\n",
    "R_of_increasing_mae, mae_of_increasing_mae = IsolateBinaryTime(optimal_NNs_mae, dataset, 'Increasing PPM', start_t_increasing, 10, verbose_isolate, \"MAE\")\n",
    "R_of_increasing_R, mae_of_increasing_R = IsolateBinaryTime(optimal_NNs_R, dataset, 'Increasing PPM', start_t_increasing, 10, verbose_isolate, \"R\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a364ff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_of_increasing_mae_1 = [R_of_increasing_mae[i][1] for i in range(len(R_of_increasing_mae))]\n",
    "R_of_increasing_mae_0 = [R_of_increasing_mae[i][0] for i in range(len(R_of_increasing_mae))]\n",
    "\n",
    "mae_of_increasing_mae_1 = [mae_of_increasing_mae[i][1] for i in range(len(mae_of_increasing_mae))]\n",
    "mae_of_increasing_mae_0 = [mae_of_increasing_mae[i][0] for i in range(len(mae_of_increasing_mae))]\n",
    "\n",
    "R_of_increasing_R_1 = [R_of_increasing_R[i][1] for i in range(len(R_of_increasing_R))]\n",
    "R_of_increasing_R_0 = [R_of_increasing_R[i][0] for i in range(len(R_of_increasing_R))]\n",
    "\n",
    "mae_of_increasing_R_1 = [mae_of_increasing_R[i][1] for i in range(len(mae_of_increasing_R))]\n",
    "mae_of_increasing_R_0 = [mae_of_increasing_R[i][0] for i in range(len(mae_of_increasing_R))]\n",
    "\n",
    "\n",
    "plt.plot(range(NN_start_sc_t, len(mae_of_increasing_mae_1) +NN_start_sc_t), mae_of_increasing_mae_1, label=\"Increasing PPM; Optimal MAE Network\")\n",
    "plt.plot(range(NN_start_sc_t, len(mae_of_increasing_mae_0) +NN_start_sc_t), mae_of_increasing_mae_0, label=\" Decreasing PPM; Optimal MAE Network\")\n",
    "\n",
    "plt.plot(range(NN_start_sc_t, len(mae_of_increasing_R_1) +NN_start_sc_t), mae_of_increasing_R_1, label=\"Increasing PPM; Optimal R Network\")\n",
    "plt.plot(range(NN_start_sc_t, len(mae_of_increasing_R_0) +NN_start_sc_t), mae_of_increasing_R_0, label=\"Decreasing PPM; Optimal R Network\")\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(NN_start_sc_t, len(R_of_increasing_mae_1) +NN_start_sc_t), R_of_increasing_mae_1, label=\"Increasing PPM; Optimal MAE Network\")\n",
    "plt.plot(range(NN_start_sc_t, len(R_of_increasing_mae_0) +NN_start_sc_t), R_of_increasing_mae_0, label=\"Decreasing PPM; Optimal MAE Network\")\n",
    "\n",
    "plt.plot(range(NN_start_sc_t, len(R_of_increasing_R_1) +NN_start_sc_t), R_of_increasing_R_1, label=\"Increasing PPM; Optimal R Network\")\n",
    "plt.plot(range(NN_start_sc_t, len(R_of_increasing_R_0) +NN_start_sc_t), R_of_increasing_R_0, label=\"Decreasing PPM; Optimal R Network\")\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('R')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea527b3",
   "metadata": {},
   "source": [
    "#### Repeat Sensor Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec297c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeatSensor(optimal_NNs, data, parameter1, parameter2, start_index, end_index, start_time, batch, vbs, mae_or_R):\n",
    "#def isolateParam(optimal_NNs, data, parameter, start_index, end_index, NN_start, batch, verbose, mae_or_R):\n",
    "\n",
    "    # Split the data labels with RSU\n",
    "    repeat_index= []\n",
    "    for i in range(start_index, end_index):\n",
    "    #for i in range(1, 4):\n",
    "\n",
    "        repeat_index.append(np.where(data[parameter1].to_numpy()  == i+1)[0])\n",
    "        #repeat_index.append(np.where(data['Repeat Sensor Use'].to_numpy()  == i)[0])\n",
    "\n",
    "    shared_tr_1 = []\n",
    "    shared_tr_2 = []\n",
    "    shared_tr_3 = []\n",
    "\n",
    "    times_index = []\n",
    "    for i in range(0, 51):\n",
    "        times_index.append(np.where(data[parameter2].to_numpy()  == i)[0].tolist())\n",
    "        #times_index.append(np.where(data['Time'].to_numpy()  == i)[0].tolist())\n",
    "\n",
    "        tr_1_tmp = []\n",
    "        tr_2_tmp = []\n",
    "        tr_3_tmp = []\n",
    "\n",
    "        for j in range(len(repeat_index)):\n",
    "    \n",
    "            for index_123 in repeat_index[j]:\n",
    "\n",
    "                if index_123 in times_index[i] and j == 0:\n",
    "                    tr_1_tmp.append(index_123)\n",
    "                elif index_123 in times_index[i] and j == 1:\n",
    "                    tr_2_tmp.append(index_123)\n",
    "                elif index_123 in times_index[i] and j == 2:\n",
    "                    tr_3_tmp.append(index_123)\n",
    "\n",
    "    #            time_0_tmp.append(index_sc)\n",
    "            \n",
    "\n",
    "        shared_tr_1.append(tr_1_tmp)\n",
    "        shared_tr_2.append(tr_2_tmp)\n",
    "        shared_tr_3.append(tr_3_tmp)\n",
    "\n",
    "    scaled_features = scaleDataset(all_features.copy())\n",
    "    #The full features of the data points that use certain time values\n",
    "    tr_features = []\n",
    "    tr_labels = []\n",
    "\n",
    "\n",
    "    for i in range(0, 51):\n",
    "        tr_features.append([\n",
    "            scaled_features.iloc[shared_tr_1[i]], \n",
    "            scaled_features.iloc[shared_tr_2[i]], \n",
    "            scaled_features.iloc[shared_tr_3[i]]\n",
    "            ])\n",
    "\n",
    "        tr_labels.append([\n",
    "            data_labels.to_numpy()[shared_tr_1[i]], \n",
    "            data_labels.to_numpy()[shared_tr_2[i]], \n",
    "            data_labels.to_numpy()[shared_tr_3[i]]\n",
    "            ])\n",
    "\n",
    "    tr_mae = []\n",
    "    tr_R = []\n",
    "    for i in range(start_time, 51):\n",
    "        tr_tmp_mae = []\n",
    "        tr_tmp_R = []\n",
    "\n",
    "        for j in range(start_index, end_index):\n",
    "\n",
    "            tmp_mae = []\n",
    "            tmp_R = []\n",
    "\n",
    "            for NN in optimal_NNs:\n",
    "                test_loss, test_mae, test_mse = NN.evaluate(tr_features[i][j], tr_labels[i][j], batch_size=batch,  verbose=vbs)\n",
    "                \n",
    "                tmp_R.append(Pearson(NN, tr_features[i][j], tr_labels[i][j], batch, vbs)[0])\n",
    "                tmp_mae.append(test_mae)\n",
    "\n",
    "                #repeat_predictions = correlation_plots(NN, repeat_labels[i], repeat_features[i], \"Testing Correlation Plot for RSU \" + str(i), \"Actual\", \"Predicted\")\n",
    "                #plotGraph(RSU_labels[i], RSU_predictions, \"RSU Plot\")\n",
    "\n",
    "            tr_tmp_mae.append(tmp_mae)\n",
    "            tr_tmp_R.append(tmp_R)\n",
    "\n",
    "        tr_mae.append(tr_tmp_mae)\n",
    "        tr_R.append(tr_tmp_R)\n",
    "\n",
    "\n",
    "    averages_mae = []\n",
    "    averages_R = []\n",
    "\n",
    "    for i in tr_mae:\n",
    "        averages_mae.append([sum(i[0])/len(i[0]), sum(i[1])/len(i[1]), sum(i[2])/len(i[2])])\n",
    "\n",
    "    for i in tr_R:\n",
    "        averages_R.append([sum(i[0])/len(i[0]), sum(i[1])/len(i[1]), sum(i[2])/len(i[2])])\n",
    "\n",
    "\n",
    "    return averages_R, averages_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cb0241",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_repeat = 100\n",
    "verbose_repeat = 0\n",
    "start_index= 0\n",
    "end_index = 3\n",
    "start_time = 20\n",
    "\n",
    "\n",
    "R_of_tr_mae, mae_of_tr_mae = repeatSensor(\n",
    "    optimal_NNs_mae, \n",
    "    dataset, \n",
    "    'Repeat Sensor Use', \n",
    "    'Time',\n",
    "    start_index, \n",
    "    end_index, \n",
    "    start_time, \n",
    "    batch_repeat, \n",
    "    verbose_repeat, \n",
    "    \"MAE\"\n",
    "    )\n",
    "\n",
    "R_of_tr_R, mae_of_tr_R = repeatSensor(\n",
    "    optimal_NNs_R, \n",
    "    dataset, \n",
    "    'Repeat Sensor Use', \n",
    "    'Time',\n",
    "    start_index, \n",
    "    end_index, \n",
    "    start_time, \n",
    "    batch_repeat, \n",
    "    verbose_repeat, \n",
    "    \"R\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caafb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_of_tr_mae_0 = [R_of_tr_mae[i][0] for i in range(len(R_of_tr_mae))]\n",
    "R_of_tr_mae_1 = [R_of_tr_mae[i][1] for i in range(len(R_of_tr_mae))]\n",
    "R_of_tr_mae_2 = [R_of_tr_mae[i][2] for i in range(len(R_of_tr_mae))]\n",
    "\n",
    "mae_of_tr_mae_0 = [mae_of_tr_mae[i][0] for i in range(len(mae_of_tr_mae))]\n",
    "mae_of_tr_mae_1 = [mae_of_tr_mae[i][1] for i in range(len(mae_of_tr_mae))]\n",
    "mae_of_tr_mae_2 = [mae_of_tr_mae[i][2] for i in range(len(mae_of_tr_mae))]\n",
    "\n",
    "R_of_tr_R_0 = [R_of_tr_R[i][0] for i in range(len(R_of_tr_R))]\n",
    "R_of_tr_R_1 = [R_of_tr_R[i][1] for i in range(len(R_of_tr_R))]\n",
    "R_of_tr_R_2 = [R_of_tr_R[i][2] for i in range(len(R_of_tr_R))]\n",
    "\n",
    "mae_of_tr_R_0 = [mae_of_tr_R[i][0] for i in range(len(mae_of_tr_R))]\n",
    "mae_of_tr_R_1 = [mae_of_tr_R[i][1] for i in range(len(mae_of_tr_R))]\n",
    "mae_of_tr_R_2 = [mae_of_tr_R[i][2] for i in range(len(mae_of_tr_R))]\n",
    "\n",
    "\n",
    "plt.plot(range(start_time, len(mae_of_tr_mae_0) +start_time), mae_of_tr_mae_0, label=\"Day 1; Optimal MAE Network\")\n",
    "plt.plot(range(start_time, len(mae_of_tr_mae_1) +start_time), mae_of_tr_mae_1, label=\" Day 2; Optimal MAE Network\")\n",
    "plt.plot(range(start_time, len(mae_of_tr_mae_2) +start_time), mae_of_tr_mae_2, label=\" Day 3; Optimal MAE Network\")\n",
    "\n",
    "plt.plot(range(start_time, len(mae_of_tr_R_0) +start_time), mae_of_tr_R_0, label=\"Day 1; Optimal R Network\")\n",
    "plt.plot(range(start_time, len(mae_of_tr_R_1) +start_time), mae_of_tr_R_1, label=\"Day 2; Optimal R Network\")\n",
    "plt.plot(range(start_time, len(mae_of_tr_R_2) +start_time), mae_of_tr_R_2, label=\"Day 3; Optimal R Network\")\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(start_time, len(R_of_tr_mae_0) +start_time), R_of_tr_mae_0, label=\"Day 1; Optimal MAE Network\")\n",
    "plt.plot(range(start_time, len(R_of_tr_mae_1) +start_time), R_of_tr_mae_1, label=\"Day 2; Optimal MAE Network\")\n",
    "plt.plot(range(start_time, len(R_of_tr_mae_2) +start_time), R_of_tr_mae_2, label=\"Day 3; Optimal MAE Network\")\n",
    "\n",
    "plt.plot(range(start_time, len(R_of_tr_R_0) +start_time), R_of_tr_R_0, label=\"Day 1; Optimal R Network\")\n",
    "plt.plot(range(start_time, len(R_of_tr_R_1) +start_time), R_of_tr_R_1, label=\"Day 2; Optimal R Network\")\n",
    "plt.plot(range(start_time, len(R_of_tr_R_2) +start_time), R_of_tr_R_2, label=\"Day 3; Optimal R Network\")\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('R')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfecd7eb",
   "metadata": {},
   "source": [
    "# Printing to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe803d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "filepath = r\".\\\\\"\n",
    "local_download_path = os.path.expanduser(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f38e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sc = pd.DataFrame({\n",
    "    \"Optimal MAE NN: R\"    : R_of_sc_mae,\n",
    "    \"Optimal R NN: R\"      : R_of_sc_R,\n",
    "    \"Optimal MAE NN: MAE\"  : mae_of_sc_mae,\n",
    "    \"Optimal R NN: MAE\"  : mae_of_sc_R\n",
    "    })\n",
    "display(dict_sc)\n",
    "dict_sc.to_csv('spin_coating.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8061156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_time = pd.DataFrame({\n",
    "    \"Optimal MAE NN: R\"    : R_time_mae, \n",
    "    \"Optimal R NN: R\"      : R_time_R,\n",
    "    \"Optimal MAE NN: MAE\"  : mae_averages_time_mae, \n",
    "    \"Optimal R NN: MAE\"    : mae_averages_time_R\n",
    "    })\n",
    "display(dict_time)\n",
    "dict_sc.to_csv('time.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1be8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sc_time = pd.DataFrame({\n",
    "    \"SC: 0;  Optimal MAE NN: R\"    : [R_of_sct_mae[i][0] for i in range(len(R_of_sct_mae))], \n",
    "    \"SC: 0;  Optimal R NN: R\"      : [R_of_sct_R[i][0] for i in range(len(R_of_sct_R))],\n",
    "\n",
    "    \"SC: 1;  Optimal MAE NN: R\"    : [R_of_sct_mae[i][1] for i in range(len(R_of_sct_mae))],     \n",
    "    \"SC: 1;  Optimal R NN: R\"      : [R_of_sct_R[i][1] for i in range(len(R_of_sct_R))],\n",
    "\n",
    "    \"SC: 0;  Optimal MAE NN: MAE\"  : [mae_of_sct_mae[i][0] for i in range(len(mae_of_sct_mae))], \n",
    "    \"SC: 0;  Optimal R NN: MAE\"    : [mae_of_sct_R[i][0] for i in range(len(mae_of_sct_R))],\n",
    "\n",
    "    \"SC: 1;  Optimal MAE NN: MAE\"  : [mae_of_sct_mae[i][1] for i in range(len(mae_of_sct_mae))], \n",
    "    \"SC: 1; Optimal R NN: MAE\"    : [mae_of_sct_R[i][1] for i in range(len(mae_of_sct_R))]\n",
    "    })\n",
    "\n",
    "display(dict_sc_time)\n",
    "dict_sc.to_csv('sc_time.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb16a674",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_repeat_time = pd.DataFrame({\n",
    "    \"Day 1;  Optimal R NN: R\"    : [R_of_tr_R[i][0] for i in range(len(R_of_tr_R))], \n",
    "    \"Day 2;  Optimal R NN: R\"    : [R_of_tr_R[i][1] for i in range(len(R_of_tr_R))], \n",
    "    \"Day 3;  Optimal R NN: R\"    : [R_of_tr_R[i][2] for i in range(len(R_of_tr_R))], \n",
    "\n",
    "    \"Day 1;  Optimal MAE NN: R\"    : [R_of_tr_mae[i][0] for i in range(len(R_of_tr_mae))], \n",
    "    \"Day 2;  Optimal MAE NN: R\"    : [R_of_tr_mae[i][1] for i in range(len(R_of_tr_mae))], \n",
    "    \"Day 3;  Optimal MAE NN: R\"    : [R_of_tr_mae[i][2] for i in range(len(R_of_tr_mae))], \n",
    "\n",
    "    \"Day 1;  Optimal R NN: MAE\"    : [mae_of_tr_R[i][0] for i in range(len(mae_of_tr_R))], \n",
    "    \"Day 2;  Optimal R NN: MAE\"    : [mae_of_tr_R[i][1] for i in range(len(mae_of_tr_R))], \n",
    "    \"Day 3;  Optimal R NN: MAE\"    : [mae_of_tr_R[i][2] for i in range(len(mae_of_tr_R))], \n",
    "\n",
    "    \"Day 1;  Optimal MAE NN: MAE\"    : [mae_of_tr_mae[i][0] for i in range(len(mae_of_tr_mae))], \n",
    "    \"Day 2;  Optimal MAE NN: MAE\"    : [mae_of_tr_mae[i][1] for i in range(len(mae_of_tr_mae))], \n",
    "    \"Day 3;  Optimal MAE NN: MAE\"    : [mae_of_tr_mae[i][2] for i in range(len(mae_of_tr_mae))], \n",
    "\n",
    "    })\n",
    "\n",
    "display(dict_sc_time)\n",
    "dict_sc.to_csv('repeat_time.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "38d5604777b095764db3d9af04f1f504aafb6d6c4fdd878aa8a69b2b09a6ca27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
